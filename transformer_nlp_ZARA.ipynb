{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:07:56.858561Z",
     "iopub.status.busy": "2021-01-08T06:07:56.857988Z",
     "iopub.status.idle": "2021-01-08T06:08:03.638618Z",
     "shell.execute_reply": "2021-01-08T06:08:03.638062Z"
    },
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "descriptions = data.iloc[:, 1].values.tolist()\n",
    "names = data.iloc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUFF SLEEVE DRESS WITH PLEATS TRF\n"
     ]
    }
   ],
   "source": [
    "print(names[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_names = []\n",
    "word_descriptions = []\n",
    "\n",
    "for name in names:\n",
    "    word_names += name.split(' ')\n",
    "\n",
    "for description in descriptions:\n",
    "    html_less = description.split('<')[0]\n",
    "    word_descriptions += html_less.split(' ')\n",
    "\n",
    "word_names = set(word_names)   \n",
    "word_descriptions = set(word_descriptions)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4487\n",
      "11218\n"
     ]
    }
   ],
   "source": [
    "print(len(word_names))  # tokens for create names -> pass to numerical and hot-encoding\n",
    "print(len(word_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self.token = {}\n",
    "        self.word  = {}\n",
    "        vocab_size = 0\n",
    "        \n",
    "    def fit(self, vocabulary):\n",
    "        n = 1\n",
    "        for word in vocabulary:\n",
    "            self.token[word] = n\n",
    "            self.word[n] = word\n",
    "            n += 1\n",
    "        self.vocab_size = n\n",
    "    \n",
    "    \"\"\"\n",
    "    def encode(self, s):\n",
    "    s = tf.compat.as_text(s)\n",
    "    tokens = self._tokenizer.tokenize(s)\n",
    "    tokens = _prepare_tokens_for_encode(tokens)\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "      ids.extend(self._token_to_ids(token))\n",
    "    return text_encoder.pad_incr(ids)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, words):\n",
    "        tokens = []\n",
    "        \n",
    "        words = tf.compat.as_text(words)\n",
    "        \n",
    "        for word in words.split(' '):\n",
    "            if word in self.token:\n",
    "                tokens += [self.token[word]]\n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "    def decode(self, tokens):\n",
    "        words = \"\"\n",
    "        for token in tokens:\n",
    "            words += self.word[token] + \" \"\n",
    "            \n",
    "        return words.strip()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:08:37.359490Z",
     "iopub.status.busy": "2021-01-08T06:08:37.358327Z",
     "iopub.status.idle": "2021-01-08T06:10:22.452743Z",
     "shell.execute_reply": "2021-01-08T06:10:22.453203Z"
    },
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tokenizer()\n",
    "tokenizer_en.fit(word_names)\n",
    "\n",
    "tokenizer_pt = tokenizer()\n",
    "tokenizer_pt.fit(word_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.458570Z",
     "iopub.status.busy": "2021-01-08T06:10:22.458002Z",
     "iopub.status.idle": "2021-01-08T06:10:22.460717Z",
     "shell.execute_reply": "2021-01-08T06:10:22.460128Z"
    },
    "id": "4DYWukNFkGQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [3277, 3091, 1196, 648, 2457, 2214, 3277]\n",
      "The original string: PUFF SLEEVE DRESS WITH PLEATS TRF PUFF\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'PUFF SLEEVE DRESS WITH PLEATS TRF PUFF'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9KJWJjrsZ4Y"
   },
   "source": [
    "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.464544Z",
     "iopub.status.busy": "2021-01-08T06:10:22.463999Z",
     "iopub.status.idle": "2021-01-08T06:10:22.467076Z",
     "shell.execute_reply": "2021-01-08T06:10:22.466638Z"
    },
    "id": "bf2ntBxjkqK6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277 ----> PUFF\n",
      "3091 ----> SLEEVE\n",
      "1196 ----> DRESS\n",
      "648 ----> WITH\n",
      "2457 ----> PLEATS\n",
      "2214 ----> TRF\n",
      "3277 ----> PUFF\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.470278Z",
     "iopub.status.busy": "2021-01-08T06:10:22.469730Z",
     "iopub.status.idle": "2021-01-08T06:10:22.471688Z",
     "shell.execute_reply": "2021-01-08T06:10:22.471282Z"
    },
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.476000Z",
     "iopub.status.busy": "2021-01-08T06:10:22.475431Z",
     "iopub.status.idle": "2021-01-08T06:10:22.477641Z",
     "shell.execute_reply": "2021-01-08T06:10:22.477135Z"
    },
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx1sFbR-9fRs"
   },
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode you can only use TensorFlow Ops and functions. \n",
    "\n",
    "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.482155Z",
     "iopub.status.busy": "2021-01-08T06:10:22.481581Z",
     "iopub.status.idle": "2021-01-08T06:10:22.483373Z",
     "shell.execute_reply": "2021-01-08T06:10:22.483811Z"
    },
    "id": "Mah1cS-P70Iz"
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JrGp5Gek6Ql"
   },
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.487366Z",
     "iopub.status.busy": "2021-01-08T06:10:22.486821Z",
     "iopub.status.idle": "2021-01-08T06:10:22.488291Z",
     "shell.execute_reply": "2021-01-08T06:10:22.488736Z"
    },
    "id": "2QEgbjntk6Yf"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.492540Z",
     "iopub.status.busy": "2021-01-08T06:10:22.491995Z",
     "iopub.status.idle": "2021-01-08T06:10:22.493939Z",
     "shell.execute_reply": "2021-01-08T06:10:22.493492Z"
    },
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = descriptions\n",
    "train_y = names\n",
    "\n",
    "valid_x = descriptions[int(len(descriptions)/2):]\n",
    "valid_y = names[int(len(names)/2):]\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = valid_data.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.613872Z",
     "iopub.status.busy": "2021-01-08T06:10:22.612937Z",
     "iopub.status.idle": "2021-01-08T06:10:22.694743Z",
     "shell.execute_reply": "2021-01-08T06:10:22.695129Z"
    },
    "id": "_fXvfYVfQr2n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 33), dtype=int64, numpy=\n",
       " array([[11219,  2966,  5710, ...,     0,     0,     0],\n",
       "        [11219,  5335,  9770, ...,     0,     0,     0],\n",
       "        [11219,  4542,  2247, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [11219,  1592,   124, ...,     0,     0,     0],\n",
       "        [11219,  1464,  9770, ...,     0,     0,     0],\n",
       "        [11219,  2966,  8059, ...,     0,     0,     0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(64, 8), dtype=int64, numpy=\n",
       " array([[4488, 2491, 3735, 4489,    0,    0,    0,    0],\n",
       "        [4488,  478,  938, 2158, 3110, 4489,    0,    0],\n",
       "        [4488, 1190, 3004, 4489,    0,    0,    0,    0],\n",
       "        [4488, 1682, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488,  377, 2785, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 3318, 3152, 3004, 4489,    0,    0,    0],\n",
       "        [4488, 4354, 3876,  648, 3436, 4489,    0,    0],\n",
       "        [4488, 2158, 3004,  648, 3983, 4489,    0,    0],\n",
       "        [4488, 2594, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 2594, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 1462, 2158,  584, 4489,    0,    0,    0],\n",
       "        [4488, 2158, 2785, 1243, 4489,    0,    0,    0],\n",
       "        [4488,  478,  938, 2158, 4165, 4489,    0,    0],\n",
       "        [4488,  478,  584, 4489,    0,    0,    0,    0],\n",
       "        [4488, 3318, 3152, 4165, 4489,    0,    0,    0],\n",
       "        [4488, 2472, 4165, 4489,    0,    0,    0,    0],\n",
       "        [4488, 2472,  584, 4489,    0,    0,    0,    0],\n",
       "        [4488, 2158, 3110,  648, 4063, 3204, 4489,    0],\n",
       "        [4488, 2158, 3735,  648, 3368, 4489,    0,    0],\n",
       "        [4488, 3318, 3152, 2158, 3876, 4489,    0,    0],\n",
       "        [4488, 1612, 3318, 3152, 1958, 4489,    0,    0],\n",
       "        [4488, 3546, 3110,  648, 3768, 3115, 4489,    0],\n",
       "        [4488,  436, 2158,  338, 3110, 4489,    0,    0],\n",
       "        [4488, 3546, 3110, 4489,    0,    0,    0,    0],\n",
       "        [4488, 4273,  338, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 1682, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 4354, 3735,  648, 3368, 4489,    0,    0],\n",
       "        [4488, 2158, 3876,  648, 2054, 2816, 4489,    0],\n",
       "        [4488,   76, 2158, 3735, 4489,    0,    0,    0],\n",
       "        [4488, 2580, 3110, 4489,    0,    0,    0,    0],\n",
       "        [4488, 3318, 3152, 2158, 3876, 4489,    0,    0],\n",
       "        [4488, 3318, 3152, 1562,  648, 1877, 4489,    0],\n",
       "        [4488, 2158, 1562,  648, 3436, 4489,    0,    0],\n",
       "        [4488, 2158, 4354, 1243, 4489,    0,    0,    0],\n",
       "        [4488, 2594, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 2158, 1562,  648,  784,  443, 2070, 4489],\n",
       "        [4488, 2158, 3876, 4489,    0,    0,    0,    0],\n",
       "        [4488, 2158,  584, 4489,    0,    0,    0,    0],\n",
       "        [4488, 1579,  584, 4489,    0,    0,    0,    0],\n",
       "        [4488, 3318, 3152,  584, 4489,    0,    0,    0],\n",
       "        [4488, 2158, 3110,  648, 1628, 1877, 4489,    0],\n",
       "        [4488, 2873, 3152, 3110,  648, 1103, 4489,    0],\n",
       "        [4488, 4165,  648, 3890, 3803, 4489,    0,    0],\n",
       "        [4488, 2158, 3126,  648, 3890, 3803, 4489,    0],\n",
       "        [4488, 3032, 3110, 4489,    0,    0,    0,    0],\n",
       "        [4488, 2158, 1196,  648, 4365, 4489,    0,    0],\n",
       "        [4488, 4442, 2158, 1243, 4489,    0,    0,    0],\n",
       "        [4488, 2873, 3152, 3004,  648, 3284, 3983, 4489],\n",
       "        [4488, 3110,  648, 4214, 4063, 4489,    0,    0],\n",
       "        [4488, 2158, 3735,  648, 4214, 4184, 4489,    0],\n",
       "        [4488, 2158, 4354, 3735,  648,  154, 4489,    0],\n",
       "        [4488, 1958,  648, 3983, 4489,    0,    0,    0],\n",
       "        [4488, 4354, 2158, 3110,  648, 4213, 1046, 4489],\n",
       "        [4488, 1774,  874, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 1175, 2158,  315, 4489,    0,    0,    0],\n",
       "        [4488, 1175, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 3211, 2158, 3110, 4489,    0,    0,    0],\n",
       "        [4488, 4331, 2158, 4165, 4489,    0,    0,    0],\n",
       "        [4488, 2158,  584,  648, 3470, 4489,    0,    0],\n",
       "        [4488, 3110,  648, 3271, 4365, 4489,    0,    0],\n",
       "        [4488,  426, 3110, 4489,    0,    0,    0,    0],\n",
       "        [4488, 3876,  648, 3284, 3590, 4489,    0,    0],\n",
       "        [4488, 1958,  648, 1752, 4489,    0,    0,    0],\n",
       "        [4488, 2158,  584,  648, 3470, 4489,    0,    0]], dtype=int64)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.699271Z",
     "iopub.status.busy": "2021-01-08T06:10:22.698643Z",
     "iopub.status.idle": "2021-01-08T06:10:22.700475Z",
     "shell.execute_reply": "2021-01-08T06:10:22.700831Z"
    },
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.706058Z",
     "iopub.status.busy": "2021-01-08T06:10:22.705481Z",
     "iopub.status.idle": "2021-01-08T06:10:22.707190Z",
     "shell.execute_reply": "2021-01-08T06:10:22.707530Z"
    },
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:22.711937Z",
     "iopub.status.busy": "2021-01-08T06:10:22.711370Z",
     "iopub.status.idle": "2021-01-08T06:10:23.324074Z",
     "shell.execute_reply": "2021-01-08T06:10:23.323528Z"
    },
    "id": "1kLCla68EloE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.328406Z",
     "iopub.status.busy": "2021-01-08T06:10:23.327788Z",
     "iopub.status.idle": "2021-01-08T06:10:23.329999Z",
     "shell.execute_reply": "2021-01-08T06:10:23.329567Z"
    },
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.333826Z",
     "iopub.status.busy": "2021-01-08T06:10:23.333253Z",
     "iopub.status.idle": "2021-01-08T06:10:23.337880Z",
     "shell.execute_reply": "2021-01-08T06:10:23.337473Z"
    },
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.341621Z",
     "iopub.status.busy": "2021-01-08T06:10:23.341054Z",
     "iopub.status.idle": "2021-01-08T06:10:23.342738Z",
     "shell.execute_reply": "2021-01-08T06:10:23.343123Z"
    },
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.346712Z",
     "iopub.status.busy": "2021-01-08T06:10:23.346143Z",
     "iopub.status.idle": "2021-01-08T06:10:23.350451Z",
     "shell.execute_reply": "2021-01-08T06:10:23.350024Z"
    },
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.356278Z",
     "iopub.status.busy": "2021-01-08T06:10:23.355713Z",
     "iopub.status.idle": "2021-01-08T06:10:23.357444Z",
     "shell.execute_reply": "2021-01-08T06:10:23.357784Z"
    },
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.361702Z",
     "iopub.status.busy": "2021-01-08T06:10:23.361094Z",
     "iopub.status.idle": "2021-01-08T06:10:23.362841Z",
     "shell.execute_reply": "2021-01-08T06:10:23.363188Z"
    },
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.368287Z",
     "iopub.status.busy": "2021-01-08T06:10:23.367721Z",
     "iopub.status.idle": "2021-01-08T06:10:23.673538Z",
     "shell.execute_reply": "2021-01-08T06:10:23.673914Z"
    },
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.679682Z",
     "iopub.status.busy": "2021-01-08T06:10:23.679088Z",
     "iopub.status.idle": "2021-01-08T06:10:23.682102Z",
     "shell.execute_reply": "2021-01-08T06:10:23.681616Z"
    },
    "id": "zg6k-fGhgXra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.686000Z",
     "iopub.status.busy": "2021-01-08T06:10:23.685384Z",
     "iopub.status.idle": "2021-01-08T06:10:23.689440Z",
     "shell.execute_reply": "2021-01-08T06:10:23.689785Z"
    },
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.694048Z",
     "iopub.status.busy": "2021-01-08T06:10:23.693439Z",
     "iopub.status.idle": "2021-01-08T06:10:23.697405Z",
     "shell.execute_reply": "2021-01-08T06:10:23.697751Z"
    },
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.707027Z",
     "iopub.status.busy": "2021-01-08T06:10:23.706471Z",
     "iopub.status.idle": "2021-01-08T06:10:23.708646Z",
     "shell.execute_reply": "2021-01-08T06:10:23.708244Z"
    },
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.722835Z",
     "iopub.status.busy": "2021-01-08T06:10:23.712420Z",
     "iopub.status.idle": "2021-01-08T06:10:23.745585Z",
     "shell.execute_reply": "2021-01-08T06:10:23.745927Z"
    },
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.750083Z",
     "iopub.status.busy": "2021-01-08T06:10:23.749537Z",
     "iopub.status.idle": "2021-01-08T06:10:23.751266Z",
     "shell.execute_reply": "2021-01-08T06:10:23.751610Z"
    },
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.757790Z",
     "iopub.status.busy": "2021-01-08T06:10:23.757230Z",
     "iopub.status.idle": "2021-01-08T06:10:23.781012Z",
     "shell.execute_reply": "2021-01-08T06:10:23.781387Z"
    },
    "id": "mytb1lPyOHLB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.788442Z",
     "iopub.status.busy": "2021-01-08T06:10:23.787861Z",
     "iopub.status.idle": "2021-01-08T06:10:23.789767Z",
     "shell.execute_reply": "2021-01-08T06:10:23.789335Z"
    },
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.800358Z",
     "iopub.status.busy": "2021-01-08T06:10:23.799796Z",
     "iopub.status.idle": "2021-01-08T06:10:23.842961Z",
     "shell.execute_reply": "2021-01-08T06:10:23.842532Z"
    },
    "id": "AzZRXdO0mI48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.847268Z",
     "iopub.status.busy": "2021-01-08T06:10:23.846689Z",
     "iopub.status.idle": "2021-01-08T06:10:23.853560Z",
     "shell.execute_reply": "2021-01-08T06:10:23.853096Z"
    },
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.865026Z",
     "iopub.status.busy": "2021-01-08T06:10:23.864430Z",
     "iopub.status.idle": "2021-01-08T06:10:23.910381Z",
     "shell.execute_reply": "2021-01-08T06:10:23.910754Z"
    },
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.917979Z",
     "iopub.status.busy": "2021-01-08T06:10:23.917412Z",
     "iopub.status.idle": "2021-01-08T06:10:23.919428Z",
     "shell.execute_reply": "2021-01-08T06:10:23.918934Z"
    },
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:23.927599Z",
     "iopub.status.busy": "2021-01-08T06:10:23.927038Z",
     "iopub.status.idle": "2021-01-08T06:10:24.192759Z",
     "shell.execute_reply": "2021-01-08T06:10:24.193139Z"
    },
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.201869Z",
     "iopub.status.busy": "2021-01-08T06:10:24.201304Z",
     "iopub.status.idle": "2021-01-08T06:10:24.202902Z",
     "shell.execute_reply": "2021-01-08T06:10:24.203254Z"
    },
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.208750Z",
     "iopub.status.busy": "2021-01-08T06:10:24.208175Z",
     "iopub.status.idle": "2021-01-08T06:10:24.405402Z",
     "shell.execute_reply": "2021-01-08T06:10:24.404842Z"
    },
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.411991Z",
     "iopub.status.busy": "2021-01-08T06:10:24.411418Z",
     "iopub.status.idle": "2021-01-08T06:10:24.413093Z",
     "shell.execute_reply": "2021-01-08T06:10:24.413442Z"
    },
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.420454Z",
     "iopub.status.busy": "2021-01-08T06:10:24.419337Z",
     "iopub.status.idle": "2021-01-08T06:10:24.893888Z",
     "shell.execute_reply": "2021-01-08T06:10:24.893401Z"
    },
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.898151Z",
     "iopub.status.busy": "2021-01-08T06:10:24.897583Z",
     "iopub.status.idle": "2021-01-08T06:10:24.899289Z",
     "shell.execute_reply": "2021-01-08T06:10:24.899635Z"
    },
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.905128Z",
     "iopub.status.busy": "2021-01-08T06:10:24.904462Z",
     "iopub.status.idle": "2021-01-08T06:10:24.906494Z",
     "shell.execute_reply": "2021-01-08T06:10:24.906089Z"
    },
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.909928Z",
     "iopub.status.busy": "2021-01-08T06:10:24.909361Z",
     "iopub.status.idle": "2021-01-08T06:10:24.911791Z",
     "shell.execute_reply": "2021-01-08T06:10:24.912140Z"
    },
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:24.916142Z",
     "iopub.status.busy": "2021-01-08T06:10:24.915556Z",
     "iopub.status.idle": "2021-01-08T06:10:25.048217Z",
     "shell.execute_reply": "2021-01-08T06:10:25.047776Z"
    },
    "id": "f33ZCgvHpPdG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.051904Z",
     "iopub.status.busy": "2021-01-08T06:10:25.051347Z",
     "iopub.status.idle": "2021-01-08T06:10:25.053070Z",
     "shell.execute_reply": "2021-01-08T06:10:25.053423Z"
    },
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.059425Z",
     "iopub.status.busy": "2021-01-08T06:10:25.058852Z",
     "iopub.status.idle": "2021-01-08T06:10:25.060747Z",
     "shell.execute_reply": "2021-01-08T06:10:25.060251Z"
    },
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "  \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.070200Z",
     "iopub.status.busy": "2021-01-08T06:10:25.069628Z",
     "iopub.status.idle": "2021-01-08T06:10:25.077395Z",
     "shell.execute_reply": "2021-01-08T06:10:25.076922Z"
    },
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.082470Z",
     "iopub.status.busy": "2021-01-08T06:10:25.081925Z",
     "iopub.status.idle": "2021-01-08T06:10:25.216072Z",
     "shell.execute_reply": "2021-01-08T06:10:25.216438Z"
    },
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.221215Z",
     "iopub.status.busy": "2021-01-08T06:10:25.220607Z",
     "iopub.status.idle": "2021-01-08T06:10:25.222787Z",
     "shell.execute_reply": "2021-01-08T06:10:25.222348Z"
    },
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.227422Z",
     "iopub.status.busy": "2021-01-08T06:10:25.226871Z",
     "iopub.status.idle": "2021-01-08T06:10:25.228661Z",
     "shell.execute_reply": "2021-01-08T06:10:25.229042Z"
    },
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# if a checkpoint exists, restore the latest checkpoint.\\nif ckpt_manager.latest_checkpoint:\\n  ckpt.restore(ckpt_manager.latest_checkpoint)\\n  print ('Latest checkpoint restored!!')\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\"\"\"\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')\n",
    "\"\"\"  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.232553Z",
     "iopub.status.busy": "2021-01-08T06:10:25.232009Z",
     "iopub.status.idle": "2021-01-08T06:10:25.234266Z",
     "shell.execute_reply": "2021-01-08T06:10:25.233734Z"
    },
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.240606Z",
     "iopub.status.busy": "2021-01-08T06:10:25.239973Z",
     "iopub.status.idle": "2021-01-08T06:10:25.242091Z",
     "shell.execute_reply": "2021-01-08T06:10:25.241660Z"
    },
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:10:25.248378Z",
     "iopub.status.busy": "2021-01-08T06:10:25.247669Z",
     "iopub.status.idle": "2021-01-08T06:21:08.163518Z",
     "shell.execute_reply": "2021-01-08T06:21:08.162991Z"
    },
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.5242 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 7.6409 Accuracy 0.1459\n",
      "Epoch 1 Batch 100 Loss 7.1173 Accuracy 0.1802\n",
      "Epoch 1 Batch 150 Loss 6.8381 Accuracy 0.1903\n",
      "Epoch 1 Batch 200 Loss 6.6104 Accuracy 0.1951\n",
      "Epoch 1 Batch 250 Loss 6.4015 Accuracy 0.2017\n",
      "Epoch 1 Batch 300 Loss 6.1933 Accuracy 0.2118\n",
      "Epoch 1 Batch 350 Loss 5.9932 Accuracy 0.2237\n",
      "Epoch 1 Batch 400 Loss 5.8130 Accuracy 0.2353\n",
      "Epoch 1 Batch 450 Loss 5.6462 Accuracy 0.2468\n",
      "Epoch 1 Batch 500 Loss 5.4968 Accuracy 0.2577\n",
      "Epoch 1 Loss 5.4357 Accuracy 0.2621\n",
      "Time taken for 1 epoch: 177.79771161079407 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.8508 Accuracy 0.3946\n",
      "Epoch 2 Batch 50 Loss 3.3985 Accuracy 0.4257\n",
      "Epoch 2 Batch 100 Loss 3.3494 Accuracy 0.4271\n",
      "Epoch 2 Batch 150 Loss 3.3712 Accuracy 0.4218\n",
      "Epoch 2 Batch 200 Loss 3.3963 Accuracy 0.4167\n",
      "Epoch 2 Batch 250 Loss 3.4251 Accuracy 0.4133\n",
      "Epoch 2 Batch 300 Loss 3.4223 Accuracy 0.4133\n",
      "Epoch 2 Batch 350 Loss 3.4135 Accuracy 0.4147\n",
      "Epoch 2 Batch 400 Loss 3.3916 Accuracy 0.4169\n",
      "Epoch 2 Batch 450 Loss 3.3675 Accuracy 0.4194\n",
      "Epoch 2 Batch 500 Loss 3.3440 Accuracy 0.4219\n",
      "Epoch 2 Loss 3.3344 Accuracy 0.4229\n",
      "Time taken for 1 epoch: 153.50798153877258 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.7621 Accuracy 0.4829\n",
      "Epoch 3 Batch 50 Loss 2.7173 Accuracy 0.4800\n",
      "Epoch 3 Batch 100 Loss 2.6958 Accuracy 0.4834\n",
      "Epoch 3 Batch 150 Loss 2.7223 Accuracy 0.4796\n",
      "Epoch 3 Batch 200 Loss 2.7495 Accuracy 0.4777\n",
      "Epoch 3 Batch 250 Loss 2.7714 Accuracy 0.4764\n",
      "Epoch 3 Batch 300 Loss 2.7871 Accuracy 0.4753\n",
      "Epoch 3 Batch 350 Loss 2.7920 Accuracy 0.4751\n",
      "Epoch 3 Batch 400 Loss 2.7890 Accuracy 0.4761\n",
      "Epoch 3 Batch 450 Loss 2.7854 Accuracy 0.4774\n",
      "Epoch 3 Batch 500 Loss 2.7792 Accuracy 0.4783\n",
      "Epoch 3 Loss 2.7767 Accuracy 0.4788\n",
      "Time taken for 1 epoch: 153.53452706336975 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.3530 Accuracy 0.5017\n",
      "Epoch 4 Batch 50 Loss 2.3578 Accuracy 0.5173\n",
      "Epoch 4 Batch 100 Loss 2.3940 Accuracy 0.5157\n",
      "Epoch 4 Batch 150 Loss 2.4217 Accuracy 0.5128\n",
      "Epoch 4 Batch 200 Loss 2.4444 Accuracy 0.5111\n",
      "Epoch 4 Batch 250 Loss 2.4729 Accuracy 0.5086\n",
      "Epoch 4 Batch 300 Loss 2.4829 Accuracy 0.5089\n",
      "Epoch 4 Batch 350 Loss 2.4874 Accuracy 0.5101\n",
      "Epoch 4 Batch 400 Loss 2.4935 Accuracy 0.5099\n",
      "Epoch 4 Batch 450 Loss 2.4925 Accuracy 0.5101\n",
      "Epoch 4 Batch 500 Loss 2.4953 Accuracy 0.5105\n",
      "Epoch 4 Loss 2.4956 Accuracy 0.5102\n",
      "Time taken for 1 epoch: 153.37241649627686 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.3845 Accuracy 0.5166\n",
      "Epoch 5 Batch 50 Loss 2.1935 Accuracy 0.5430\n",
      "Epoch 5 Batch 100 Loss 2.2157 Accuracy 0.5389\n",
      "Epoch 5 Batch 150 Loss 2.2123 Accuracy 0.5382\n",
      "Epoch 5 Batch 200 Loss 2.2467 Accuracy 0.5354\n",
      "Epoch 5 Batch 250 Loss 2.2669 Accuracy 0.5336\n",
      "Epoch 5 Batch 300 Loss 2.2829 Accuracy 0.5327\n",
      "Epoch 5 Batch 350 Loss 2.2991 Accuracy 0.5315\n",
      "Epoch 5 Batch 400 Loss 2.3116 Accuracy 0.5298\n",
      "Epoch 5 Batch 450 Loss 2.3156 Accuracy 0.5301\n",
      "Epoch 5 Batch 500 Loss 2.3206 Accuracy 0.5296\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-1\n",
      "Epoch 5 Loss 2.3220 Accuracy 0.5292\n",
      "Time taken for 1 epoch: 159.98092436790466 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.0838 Accuracy 0.5350\n",
      "Epoch 6 Batch 50 Loss 2.1100 Accuracy 0.5439\n",
      "Epoch 6 Batch 100 Loss 2.1410 Accuracy 0.5414\n",
      "Epoch 6 Batch 150 Loss 2.1585 Accuracy 0.5388\n",
      "Epoch 6 Batch 200 Loss 2.1702 Accuracy 0.5398\n",
      "Epoch 6 Batch 250 Loss 2.1887 Accuracy 0.5397\n",
      "Epoch 6 Batch 300 Loss 2.2001 Accuracy 0.5395\n",
      "Epoch 6 Batch 350 Loss 2.2117 Accuracy 0.5384\n",
      "Epoch 6 Batch 400 Loss 2.2181 Accuracy 0.5383\n",
      "Epoch 6 Batch 450 Loss 2.2230 Accuracy 0.5387\n",
      "Epoch 6 Batch 500 Loss 2.2308 Accuracy 0.5382\n",
      "Epoch 6 Loss 2.2295 Accuracy 0.5386\n",
      "Time taken for 1 epoch: 152.93930435180664 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.8737 Accuracy 0.5493\n",
      "Epoch 7 Batch 50 Loss 2.0277 Accuracy 0.5539\n",
      "Epoch 7 Batch 100 Loss 2.0383 Accuracy 0.5549\n",
      "Epoch 7 Batch 150 Loss 2.0560 Accuracy 0.5531\n",
      "Epoch 7 Batch 200 Loss 2.0857 Accuracy 0.5502\n",
      "Epoch 7 Batch 250 Loss 2.1186 Accuracy 0.5473\n",
      "Epoch 7 Batch 300 Loss 2.1328 Accuracy 0.5457\n",
      "Epoch 7 Batch 350 Loss 2.1434 Accuracy 0.5448\n",
      "Epoch 7 Batch 400 Loss 2.1556 Accuracy 0.5433\n",
      "Epoch 7 Batch 450 Loss 2.1672 Accuracy 0.5423\n",
      "Epoch 7 Batch 500 Loss 2.1752 Accuracy 0.5415\n",
      "Epoch 7 Loss 2.1744 Accuracy 0.5418\n",
      "Time taken for 1 epoch: 153.5781421661377 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.0644 Accuracy 0.5211\n",
      "Epoch 8 Batch 50 Loss 1.9931 Accuracy 0.5569\n",
      "Epoch 8 Batch 100 Loss 2.0317 Accuracy 0.5500\n",
      "Epoch 8 Batch 150 Loss 2.0645 Accuracy 0.5465\n",
      "Epoch 8 Batch 200 Loss 2.1015 Accuracy 0.5426\n",
      "Epoch 8 Batch 250 Loss 2.1302 Accuracy 0.5394\n",
      "Epoch 8 Batch 300 Loss 2.1525 Accuracy 0.5365\n",
      "Epoch 8 Batch 350 Loss 2.1542 Accuracy 0.5374\n",
      "Epoch 8 Batch 400 Loss 2.1581 Accuracy 0.5380\n",
      "Epoch 8 Batch 450 Loss 2.1633 Accuracy 0.5379\n",
      "Epoch 8 Batch 500 Loss 2.1729 Accuracy 0.5373\n",
      "Epoch 8 Loss 2.1763 Accuracy 0.5367\n",
      "Time taken for 1 epoch: 153.8194670677185 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.1341 Accuracy 0.5284\n",
      "Epoch 9 Batch 50 Loss 2.0118 Accuracy 0.5486\n",
      "Epoch 9 Batch 100 Loss 2.0079 Accuracy 0.5543\n",
      "Epoch 9 Batch 150 Loss 2.0030 Accuracy 0.5551\n",
      "Epoch 9 Batch 200 Loss 2.0465 Accuracy 0.5488\n",
      "Epoch 9 Batch 250 Loss 2.0677 Accuracy 0.5463\n",
      "Epoch 9 Batch 300 Loss 2.0815 Accuracy 0.5449\n",
      "Epoch 9 Batch 350 Loss 2.0983 Accuracy 0.5431\n",
      "Epoch 9 Batch 400 Loss 2.0963 Accuracy 0.5442\n",
      "Epoch 9 Batch 450 Loss 2.0897 Accuracy 0.5468\n",
      "Epoch 9 Batch 500 Loss 2.0924 Accuracy 0.5472\n",
      "Epoch 9 Loss 2.0917 Accuracy 0.5474\n",
      "Time taken for 1 epoch: 154.04408478736877 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.8759 Accuracy 0.5739\n",
      "Epoch 10 Batch 50 Loss 1.8768 Accuracy 0.5711\n",
      "Epoch 10 Batch 100 Loss 1.8840 Accuracy 0.5710\n",
      "Epoch 10 Batch 150 Loss 1.8850 Accuracy 0.5717\n",
      "Epoch 10 Batch 200 Loss 1.8933 Accuracy 0.5714\n",
      "Epoch 10 Batch 250 Loss 1.9036 Accuracy 0.5709\n",
      "Epoch 10 Batch 300 Loss 1.9043 Accuracy 0.5721\n",
      "Epoch 10 Batch 350 Loss 1.9035 Accuracy 0.5729\n",
      "Epoch 10 Batch 400 Loss 1.9033 Accuracy 0.5741\n",
      "Epoch 10 Batch 450 Loss 1.9034 Accuracy 0.5753\n",
      "Epoch 10 Batch 500 Loss 1.9093 Accuracy 0.5751\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train\\ckpt-2\n",
      "Epoch 10 Loss 1.9094 Accuracy 0.5754\n",
      "Time taken for 1 epoch: 160.50477385520935 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.8082 Accuracy 0.5642\n",
      "Epoch 11 Batch 50 Loss 1.8017 Accuracy 0.5882\n",
      "Epoch 11 Batch 100 Loss 1.7901 Accuracy 0.5909\n",
      "Epoch 11 Batch 150 Loss 1.7813 Accuracy 0.5925\n",
      "Epoch 11 Batch 200 Loss 1.7875 Accuracy 0.5926\n",
      "Epoch 11 Batch 250 Loss 1.7884 Accuracy 0.5937\n",
      "Epoch 11 Batch 300 Loss 1.7858 Accuracy 0.5951\n",
      "Epoch 11 Batch 350 Loss 1.7831 Accuracy 0.5954\n",
      "Epoch 11 Batch 400 Loss 1.7854 Accuracy 0.5953\n",
      "Epoch 11 Batch 450 Loss 1.7860 Accuracy 0.5957\n",
      "Epoch 11 Batch 500 Loss 1.7899 Accuracy 0.5955\n",
      "Epoch 11 Loss 1.7911 Accuracy 0.5952\n",
      "Time taken for 1 epoch: 153.81512022018433 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.7632 Accuracy 0.6055\n",
      "Epoch 12 Batch 50 Loss 1.6727 Accuracy 0.6002\n",
      "Epoch 12 Batch 100 Loss 1.6720 Accuracy 0.6020\n",
      "Epoch 12 Batch 150 Loss 1.6608 Accuracy 0.6057\n",
      "Epoch 12 Batch 200 Loss 1.6570 Accuracy 0.6080\n",
      "Epoch 12 Batch 250 Loss 1.6557 Accuracy 0.6090\n",
      "Epoch 12 Batch 300 Loss 1.6596 Accuracy 0.6094\n",
      "Epoch 12 Batch 350 Loss 1.6650 Accuracy 0.6095\n",
      "Epoch 12 Batch 400 Loss 1.6677 Accuracy 0.6101\n",
      "Epoch 12 Batch 450 Loss 1.6687 Accuracy 0.6104\n",
      "Epoch 12 Batch 500 Loss 1.6667 Accuracy 0.6124\n",
      "Epoch 12 Loss 1.6670 Accuracy 0.6126\n",
      "Time taken for 1 epoch: 153.21080374717712 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.5215 Accuracy 0.6393\n",
      "Epoch 13 Batch 50 Loss 1.5810 Accuracy 0.6175\n",
      "Epoch 13 Batch 100 Loss 1.5720 Accuracy 0.6173\n",
      "Epoch 13 Batch 150 Loss 1.5577 Accuracy 0.6214\n",
      "Epoch 13 Batch 200 Loss 1.5540 Accuracy 0.6241\n",
      "Epoch 13 Batch 250 Loss 1.5524 Accuracy 0.6264\n",
      "Epoch 13 Batch 300 Loss 1.5514 Accuracy 0.6278\n",
      "Epoch 13 Batch 350 Loss 1.5521 Accuracy 0.6292\n",
      "Epoch 13 Batch 400 Loss 1.5570 Accuracy 0.6288\n",
      "Epoch 13 Batch 450 Loss 1.5577 Accuracy 0.6299\n",
      "Epoch 13 Batch 500 Loss 1.5604 Accuracy 0.6306\n",
      "Epoch 13 Loss 1.5627 Accuracy 0.6304\n",
      "Time taken for 1 epoch: 152.6390471458435 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.3893 Accuracy 0.6401\n",
      "Epoch 14 Batch 50 Loss 1.4562 Accuracy 0.6410\n",
      "Epoch 14 Batch 100 Loss 1.4699 Accuracy 0.6400\n",
      "Epoch 14 Batch 150 Loss 1.4692 Accuracy 0.6405\n",
      "Epoch 14 Batch 200 Loss 1.4690 Accuracy 0.6418\n",
      "Epoch 14 Batch 250 Loss 1.4689 Accuracy 0.6425\n",
      "Epoch 14 Batch 300 Loss 1.4641 Accuracy 0.6446\n",
      "Epoch 14 Batch 350 Loss 1.4626 Accuracy 0.6460\n",
      "Epoch 14 Batch 400 Loss 1.4643 Accuracy 0.6465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 450 Loss 1.4703 Accuracy 0.6465\n",
      "Epoch 14 Batch 500 Loss 1.4711 Accuracy 0.6470\n",
      "Epoch 14 Loss 1.4700 Accuracy 0.6472\n",
      "Time taken for 1 epoch: 153.37371277809143 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.4094 Accuracy 0.6169\n",
      "Epoch 15 Batch 50 Loss 1.4009 Accuracy 0.6441\n",
      "Epoch 15 Batch 100 Loss 1.4054 Accuracy 0.6454\n",
      "Epoch 15 Batch 150 Loss 1.3850 Accuracy 0.6496\n",
      "Epoch 15 Batch 200 Loss 1.3727 Accuracy 0.6540\n",
      "Epoch 15 Batch 250 Loss 1.3725 Accuracy 0.6561\n",
      "Epoch 15 Batch 300 Loss 1.3663 Accuracy 0.6590\n",
      "Epoch 15 Batch 350 Loss 1.3642 Accuracy 0.6612\n",
      "Epoch 15 Batch 400 Loss 1.3626 Accuracy 0.6622\n",
      "Epoch 15 Batch 450 Loss 1.3657 Accuracy 0.6625\n",
      "Epoch 15 Batch 500 Loss 1.3701 Accuracy 0.6628\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train\\ckpt-3\n",
      "Epoch 15 Loss 1.3713 Accuracy 0.6631\n",
      "Time taken for 1 epoch: 160.2608814239502 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.1730 Accuracy 0.7003\n",
      "Epoch 16 Batch 50 Loss 1.2940 Accuracy 0.6699\n",
      "Epoch 16 Batch 100 Loss 1.3192 Accuracy 0.6626\n",
      "Epoch 16 Batch 150 Loss 1.3047 Accuracy 0.6660\n",
      "Epoch 16 Batch 200 Loss 1.2918 Accuracy 0.6714\n",
      "Epoch 16 Batch 250 Loss 1.2824 Accuracy 0.6746\n",
      "Epoch 16 Batch 300 Loss 1.2779 Accuracy 0.6767\n",
      "Epoch 16 Batch 350 Loss 1.2814 Accuracy 0.6773\n",
      "Epoch 16 Batch 400 Loss 1.2825 Accuracy 0.6784\n",
      "Epoch 16 Batch 450 Loss 1.2881 Accuracy 0.6782\n",
      "Epoch 16 Batch 500 Loss 1.2958 Accuracy 0.6775\n",
      "Epoch 16 Loss 1.2977 Accuracy 0.6772\n",
      "Time taken for 1 epoch: 153.18263506889343 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2614 Accuracy 0.6595\n",
      "Epoch 17 Batch 50 Loss 1.2591 Accuracy 0.6682\n",
      "Epoch 17 Batch 100 Loss 1.2562 Accuracy 0.6707\n",
      "Epoch 17 Batch 150 Loss 1.2408 Accuracy 0.6760\n",
      "Epoch 17 Batch 200 Loss 1.2281 Accuracy 0.6812\n",
      "Epoch 17 Batch 250 Loss 1.2223 Accuracy 0.6851\n",
      "Epoch 17 Batch 300 Loss 1.2194 Accuracy 0.6864\n",
      "Epoch 17 Batch 350 Loss 1.2175 Accuracy 0.6879\n",
      "Epoch 17 Batch 400 Loss 1.2232 Accuracy 0.6880\n",
      "Epoch 17 Batch 450 Loss 1.2274 Accuracy 0.6885\n",
      "Epoch 17 Batch 500 Loss 1.2314 Accuracy 0.6885\n",
      "Epoch 17 Loss 1.2336 Accuracy 0.6883\n",
      "Time taken for 1 epoch: 153.76766729354858 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1312 Accuracy 0.6644\n",
      "Epoch 18 Batch 50 Loss 1.1723 Accuracy 0.6879\n",
      "Epoch 18 Batch 100 Loss 1.1811 Accuracy 0.6881\n",
      "Epoch 18 Batch 150 Loss 1.1816 Accuracy 0.6901\n",
      "Epoch 18 Batch 200 Loss 1.1690 Accuracy 0.6937\n",
      "Epoch 18 Batch 250 Loss 1.1547 Accuracy 0.6985\n",
      "Epoch 18 Batch 300 Loss 1.1509 Accuracy 0.6997\n",
      "Epoch 18 Batch 350 Loss 1.1510 Accuracy 0.7017\n",
      "Epoch 18 Batch 400 Loss 1.1542 Accuracy 0.7015\n",
      "Epoch 18 Batch 450 Loss 1.1582 Accuracy 0.7014\n",
      "Epoch 18 Batch 500 Loss 1.1624 Accuracy 0.7011\n",
      "Epoch 18 Loss 1.1645 Accuracy 0.7009\n",
      "Time taken for 1 epoch: 153.1000006198883 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.0428 Accuracy 0.6997\n",
      "Epoch 19 Batch 50 Loss 1.1657 Accuracy 0.6892\n",
      "Epoch 19 Batch 100 Loss 1.1592 Accuracy 0.6924\n",
      "Epoch 19 Batch 150 Loss 1.1376 Accuracy 0.6973\n",
      "Epoch 19 Batch 200 Loss 1.1176 Accuracy 0.7032\n",
      "Epoch 19 Batch 250 Loss 1.1030 Accuracy 0.7067\n",
      "Epoch 19 Batch 300 Loss 1.0974 Accuracy 0.7090\n",
      "Epoch 19 Batch 350 Loss 1.0962 Accuracy 0.7104\n",
      "Epoch 19 Batch 400 Loss 1.0973 Accuracy 0.7119\n",
      "Epoch 19 Batch 450 Loss 1.1006 Accuracy 0.7125\n",
      "Epoch 19 Batch 500 Loss 1.1015 Accuracy 0.7131\n",
      "Epoch 19 Loss 1.1036 Accuracy 0.7128\n",
      "Time taken for 1 epoch: 153.401469707489 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0962 Accuracy 0.7057\n",
      "Epoch 20 Batch 50 Loss 1.0767 Accuracy 0.7084\n",
      "Epoch 20 Batch 100 Loss 1.0798 Accuracy 0.7086\n",
      "Epoch 20 Batch 150 Loss 1.0672 Accuracy 0.7112\n",
      "Epoch 20 Batch 200 Loss 1.0535 Accuracy 0.7161\n",
      "Epoch 20 Batch 250 Loss 1.0448 Accuracy 0.7193\n",
      "Epoch 20 Batch 300 Loss 1.0370 Accuracy 0.7224\n",
      "Epoch 20 Batch 350 Loss 1.0353 Accuracy 0.7238\n",
      "Epoch 20 Batch 400 Loss 1.0349 Accuracy 0.7247\n",
      "Epoch 20 Batch 450 Loss 1.0347 Accuracy 0.7254\n",
      "Epoch 20 Batch 500 Loss 1.0404 Accuracy 0.7251\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train\\ckpt-4\n",
      "Epoch 20 Loss 1.0420 Accuracy 0.7250\n",
      "Time taken for 1 epoch: 161.05024886131287 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.9408 Accuracy 0.7491\n",
      "Epoch 21 Batch 50 Loss 1.0219 Accuracy 0.7177\n",
      "Epoch 21 Batch 100 Loss 1.0255 Accuracy 0.7191\n",
      "Epoch 21 Batch 150 Loss 1.0123 Accuracy 0.7233\n",
      "Epoch 21 Batch 200 Loss 1.0005 Accuracy 0.7277\n",
      "Epoch 21 Batch 250 Loss 0.9907 Accuracy 0.7320\n",
      "Epoch 21 Batch 300 Loss 0.9854 Accuracy 0.7340\n",
      "Epoch 21 Batch 350 Loss 0.9842 Accuracy 0.7345\n",
      "Epoch 21 Batch 400 Loss 0.9855 Accuracy 0.7349\n",
      "Epoch 21 Batch 450 Loss 0.9873 Accuracy 0.7356\n",
      "Epoch 21 Batch 500 Loss 0.9942 Accuracy 0.7349\n",
      "Epoch 21 Loss 0.9962 Accuracy 0.7345\n",
      "Time taken for 1 epoch: 153.3268027305603 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.8821 Accuracy 0.7559\n",
      "Epoch 22 Batch 50 Loss 0.9974 Accuracy 0.7233\n",
      "Epoch 22 Batch 100 Loss 0.9948 Accuracy 0.7260\n",
      "Epoch 22 Batch 150 Loss 0.9807 Accuracy 0.7291\n",
      "Epoch 22 Batch 200 Loss 0.9621 Accuracy 0.7348\n",
      "Epoch 22 Batch 250 Loss 0.9496 Accuracy 0.7393\n",
      "Epoch 22 Batch 300 Loss 0.9457 Accuracy 0.7427\n",
      "Epoch 22 Batch 350 Loss 0.9414 Accuracy 0.7445\n",
      "Epoch 22 Batch 400 Loss 0.9428 Accuracy 0.7452\n",
      "Epoch 22 Batch 450 Loss 0.9447 Accuracy 0.7457\n",
      "Epoch 22 Batch 500 Loss 0.9499 Accuracy 0.7453\n",
      "Epoch 22 Loss 0.9505 Accuracy 0.7455\n",
      "Time taken for 1 epoch: 155.44754600524902 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.0677 Accuracy 0.7063\n",
      "Epoch 23 Batch 50 Loss 0.9591 Accuracy 0.7337\n",
      "Epoch 23 Batch 100 Loss 0.9527 Accuracy 0.7351\n",
      "Epoch 23 Batch 150 Loss 0.9395 Accuracy 0.7396\n",
      "Epoch 23 Batch 200 Loss 0.9211 Accuracy 0.7457\n",
      "Epoch 23 Batch 250 Loss 0.9085 Accuracy 0.7502\n",
      "Epoch 23 Batch 300 Loss 0.9032 Accuracy 0.7525\n",
      "Epoch 23 Batch 350 Loss 0.9016 Accuracy 0.7536\n",
      "Epoch 23 Batch 400 Loss 0.9002 Accuracy 0.7546\n",
      "Epoch 23 Batch 450 Loss 0.9014 Accuracy 0.7551\n",
      "Epoch 23 Batch 500 Loss 0.9023 Accuracy 0.7554\n",
      "Epoch 23 Loss 0.9064 Accuracy 0.7550\n",
      "Time taken for 1 epoch: 154.45619583129883 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.9569 Accuracy 0.7263\n",
      "Epoch 24 Batch 50 Loss 0.9040 Accuracy 0.7449\n",
      "Epoch 24 Batch 100 Loss 0.8999 Accuracy 0.7462\n",
      "Epoch 24 Batch 150 Loss 0.8805 Accuracy 0.7521\n",
      "Epoch 24 Batch 200 Loss 0.8718 Accuracy 0.7549\n",
      "Epoch 24 Batch 250 Loss 0.8566 Accuracy 0.7607\n",
      "Epoch 24 Batch 300 Loss 0.8521 Accuracy 0.7639\n",
      "Epoch 24 Batch 350 Loss 0.8520 Accuracy 0.7644\n",
      "Epoch 24 Batch 400 Loss 0.8539 Accuracy 0.7647\n",
      "Epoch 24 Batch 450 Loss 0.8559 Accuracy 0.7653\n",
      "Epoch 24 Batch 500 Loss 0.8597 Accuracy 0.7648\n",
      "Epoch 24 Loss 0.8599 Accuracy 0.7649\n",
      "Time taken for 1 epoch: 154.39662504196167 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.7078 Accuracy 0.8121\n",
      "Epoch 25 Batch 50 Loss 0.8662 Accuracy 0.7551\n",
      "Epoch 25 Batch 100 Loss 0.8683 Accuracy 0.7513\n",
      "Epoch 25 Batch 150 Loss 0.8586 Accuracy 0.7554\n",
      "Epoch 25 Batch 200 Loss 0.8387 Accuracy 0.7619\n",
      "Epoch 25 Batch 250 Loss 0.8266 Accuracy 0.7661\n",
      "Epoch 25 Batch 300 Loss 0.8216 Accuracy 0.7680\n",
      "Epoch 25 Batch 350 Loss 0.8204 Accuracy 0.7694\n",
      "Epoch 25 Batch 400 Loss 0.8180 Accuracy 0.7710\n",
      "Epoch 25 Batch 450 Loss 0.8178 Accuracy 0.7723\n",
      "Epoch 25 Batch 500 Loss 0.8216 Accuracy 0.7719\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train\\ckpt-5\n",
      "Epoch 25 Loss 0.8224 Accuracy 0.7719\n",
      "Time taken for 1 epoch: 161.32766962051392 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.6840 Accuracy 0.7966\n",
      "Epoch 26 Batch 50 Loss 0.8156 Accuracy 0.7669\n",
      "Epoch 26 Batch 100 Loss 0.8184 Accuracy 0.7677\n",
      "Epoch 26 Batch 150 Loss 0.8074 Accuracy 0.7706\n",
      "Epoch 26 Batch 200 Loss 0.7848 Accuracy 0.7768\n",
      "Epoch 26 Batch 250 Loss 0.7750 Accuracy 0.7805\n",
      "Epoch 26 Batch 300 Loss 0.7750 Accuracy 0.7815\n",
      "Epoch 26 Batch 350 Loss 0.7750 Accuracy 0.7828\n",
      "Epoch 26 Batch 400 Loss 0.7775 Accuracy 0.7828\n",
      "Epoch 26 Batch 450 Loss 0.7792 Accuracy 0.7828\n",
      "Epoch 26 Batch 500 Loss 0.7816 Accuracy 0.7824\n",
      "Epoch 26 Loss 0.7845 Accuracy 0.7820\n",
      "Time taken for 1 epoch: 153.513245344162 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.8181 Accuracy 0.7544\n",
      "Epoch 27 Batch 50 Loss 0.7891 Accuracy 0.7678\n",
      "Epoch 27 Batch 100 Loss 0.7928 Accuracy 0.7702\n",
      "Epoch 27 Batch 150 Loss 0.7836 Accuracy 0.7749\n",
      "Epoch 27 Batch 200 Loss 0.7571 Accuracy 0.7827\n",
      "Epoch 27 Batch 250 Loss 0.7467 Accuracy 0.7861\n",
      "Epoch 27 Batch 300 Loss 0.7411 Accuracy 0.7886\n",
      "Epoch 27 Batch 350 Loss 0.7425 Accuracy 0.7890\n",
      "Epoch 27 Batch 400 Loss 0.7423 Accuracy 0.7895\n",
      "Epoch 27 Batch 450 Loss 0.7447 Accuracy 0.7897\n",
      "Epoch 27 Batch 500 Loss 0.7483 Accuracy 0.7896\n",
      "Epoch 27 Loss 0.7498 Accuracy 0.7894\n",
      "Time taken for 1 epoch: 155.34237957000732 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.7745 Accuracy 0.7474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 50 Loss 0.7768 Accuracy 0.7741\n",
      "Epoch 28 Batch 100 Loss 0.7531 Accuracy 0.7811\n",
      "Epoch 28 Batch 150 Loss 0.7371 Accuracy 0.7858\n",
      "Epoch 28 Batch 200 Loss 0.7212 Accuracy 0.7916\n",
      "Epoch 28 Batch 250 Loss 0.7111 Accuracy 0.7954\n",
      "Epoch 28 Batch 300 Loss 0.7087 Accuracy 0.7968\n",
      "Epoch 28 Batch 350 Loss 0.7083 Accuracy 0.7974\n",
      "Epoch 28 Batch 400 Loss 0.7076 Accuracy 0.7983\n",
      "Epoch 28 Batch 450 Loss 0.7095 Accuracy 0.7979\n",
      "Epoch 28 Batch 500 Loss 0.7138 Accuracy 0.7976\n",
      "Epoch 28 Loss 0.7168 Accuracy 0.7970\n",
      "Time taken for 1 epoch: 161.55757689476013 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.6353 Accuracy 0.8066\n",
      "Epoch 29 Batch 50 Loss 0.7396 Accuracy 0.7854\n",
      "Epoch 29 Batch 100 Loss 0.7382 Accuracy 0.7844\n",
      "Epoch 29 Batch 150 Loss 0.7200 Accuracy 0.7899\n",
      "Epoch 29 Batch 200 Loss 0.7015 Accuracy 0.7959\n",
      "Epoch 29 Batch 250 Loss 0.6846 Accuracy 0.8019\n",
      "Epoch 29 Batch 300 Loss 0.6795 Accuracy 0.8040\n",
      "Epoch 29 Batch 350 Loss 0.6793 Accuracy 0.8047\n",
      "Epoch 29 Batch 400 Loss 0.6818 Accuracy 0.8046\n",
      "Epoch 29 Batch 450 Loss 0.6843 Accuracy 0.8043\n",
      "Epoch 29 Batch 500 Loss 0.6883 Accuracy 0.8044\n",
      "Epoch 29 Loss 0.6895 Accuracy 0.8043\n",
      "Time taken for 1 epoch: 172.38005113601685 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.7060 Accuracy 0.8028\n",
      "Epoch 30 Batch 50 Loss 0.7067 Accuracy 0.7922\n",
      "Epoch 30 Batch 100 Loss 0.7046 Accuracy 0.7922\n",
      "Epoch 30 Batch 150 Loss 0.6937 Accuracy 0.7963\n",
      "Epoch 30 Batch 200 Loss 0.6732 Accuracy 0.8038\n",
      "Epoch 30 Batch 250 Loss 0.6609 Accuracy 0.8079\n",
      "Epoch 30 Batch 300 Loss 0.6545 Accuracy 0.8104\n",
      "Epoch 30 Batch 350 Loss 0.6528 Accuracy 0.8112\n",
      "Epoch 30 Batch 400 Loss 0.6538 Accuracy 0.8116\n",
      "Epoch 30 Batch 450 Loss 0.6572 Accuracy 0.8111\n",
      "Epoch 30 Batch 500 Loss 0.6601 Accuracy 0.8109\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train\\ckpt-6\n",
      "Epoch 30 Loss 0.6616 Accuracy 0.8109\n",
      "Time taken for 1 epoch: 175.00428342819214 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.7168 Accuracy 0.7616\n",
      "Epoch 31 Batch 50 Loss 0.6898 Accuracy 0.7976\n",
      "Epoch 31 Batch 100 Loss 0.6769 Accuracy 0.7981\n",
      "Epoch 31 Batch 150 Loss 0.6600 Accuracy 0.8046\n",
      "Epoch 31 Batch 200 Loss 0.6402 Accuracy 0.8111\n",
      "Epoch 31 Batch 250 Loss 0.6298 Accuracy 0.8143\n",
      "Epoch 31 Batch 300 Loss 0.6239 Accuracy 0.8164\n",
      "Epoch 31 Batch 350 Loss 0.6214 Accuracy 0.8181\n",
      "Epoch 31 Batch 400 Loss 0.6263 Accuracy 0.8174\n",
      "Epoch 31 Batch 450 Loss 0.6299 Accuracy 0.8170\n",
      "Epoch 31 Batch 500 Loss 0.6322 Accuracy 0.8169\n",
      "Epoch 31 Loss 0.6342 Accuracy 0.8167\n",
      "Time taken for 1 epoch: 180.4172990322113 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.6082 Accuracy 0.8048\n",
      "Epoch 32 Batch 50 Loss 0.6597 Accuracy 0.8049\n",
      "Epoch 32 Batch 100 Loss 0.6617 Accuracy 0.8031\n",
      "Epoch 32 Batch 150 Loss 0.6439 Accuracy 0.8087\n",
      "Epoch 32 Batch 200 Loss 0.6274 Accuracy 0.8152\n",
      "Epoch 32 Batch 250 Loss 0.6095 Accuracy 0.8203\n",
      "Epoch 32 Batch 300 Loss 0.6070 Accuracy 0.8223\n",
      "Epoch 32 Batch 350 Loss 0.6055 Accuracy 0.8236\n",
      "Epoch 32 Batch 400 Loss 0.6061 Accuracy 0.8239\n",
      "Epoch 32 Batch 450 Loss 0.6078 Accuracy 0.8238\n",
      "Epoch 32 Batch 500 Loss 0.6090 Accuracy 0.8240\n",
      "Epoch 32 Loss 0.6102 Accuracy 0.8239\n",
      "Time taken for 1 epoch: 157.23140621185303 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.5192 Accuracy 0.8350\n",
      "Epoch 33 Batch 50 Loss 0.6211 Accuracy 0.8117\n",
      "Epoch 33 Batch 100 Loss 0.6262 Accuracy 0.8106\n",
      "Epoch 33 Batch 150 Loss 0.6097 Accuracy 0.8164\n",
      "Epoch 33 Batch 200 Loss 0.5915 Accuracy 0.8234\n",
      "Epoch 33 Batch 250 Loss 0.5821 Accuracy 0.8269\n",
      "Epoch 33 Batch 300 Loss 0.5776 Accuracy 0.8283\n",
      "Epoch 33 Batch 350 Loss 0.5746 Accuracy 0.8297\n",
      "Epoch 33 Batch 400 Loss 0.5734 Accuracy 0.8305\n",
      "Epoch 33 Batch 450 Loss 0.5784 Accuracy 0.8297\n",
      "Epoch 33 Batch 500 Loss 0.5822 Accuracy 0.8294\n",
      "Epoch 33 Loss 0.5841 Accuracy 0.8290\n",
      "Time taken for 1 epoch: 155.27390933036804 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.5860 Accuracy 0.8091\n",
      "Epoch 34 Batch 50 Loss 0.6115 Accuracy 0.8148\n",
      "Epoch 34 Batch 100 Loss 0.6011 Accuracy 0.8166\n",
      "Epoch 34 Batch 150 Loss 0.5905 Accuracy 0.8212\n",
      "Epoch 34 Batch 200 Loss 0.5725 Accuracy 0.8280\n",
      "Epoch 34 Batch 250 Loss 0.5599 Accuracy 0.8326\n",
      "Epoch 34 Batch 300 Loss 0.5541 Accuracy 0.8352\n",
      "Epoch 34 Batch 350 Loss 0.5486 Accuracy 0.8377\n",
      "Epoch 34 Batch 400 Loss 0.5511 Accuracy 0.8377\n",
      "Epoch 34 Batch 450 Loss 0.5541 Accuracy 0.8375\n",
      "Epoch 34 Batch 500 Loss 0.5560 Accuracy 0.8375\n",
      "Epoch 34 Loss 0.5572 Accuracy 0.8374\n",
      "Time taken for 1 epoch: 155.39869213104248 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.6759 Accuracy 0.7845\n",
      "Epoch 35 Batch 50 Loss 0.5839 Accuracy 0.8246\n",
      "Epoch 35 Batch 100 Loss 0.5825 Accuracy 0.8251\n",
      "Epoch 35 Batch 150 Loss 0.5687 Accuracy 0.8287\n",
      "Epoch 35 Batch 200 Loss 0.5538 Accuracy 0.8338\n",
      "Epoch 35 Batch 250 Loss 0.5435 Accuracy 0.8372\n",
      "Epoch 35 Batch 300 Loss 0.5349 Accuracy 0.8406\n",
      "Epoch 35 Batch 350 Loss 0.5358 Accuracy 0.8410\n",
      "Epoch 35 Batch 400 Loss 0.5375 Accuracy 0.8410\n",
      "Epoch 35 Batch 450 Loss 0.5403 Accuracy 0.8409\n",
      "Epoch 35 Batch 500 Loss 0.5440 Accuracy 0.8404\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train\\ckpt-7\n",
      "Epoch 35 Loss 0.5450 Accuracy 0.8401\n",
      "Time taken for 1 epoch: 163.24529385566711 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.4866 Accuracy 0.8586\n",
      "Epoch 36 Batch 50 Loss 0.5699 Accuracy 0.8270\n",
      "Epoch 36 Batch 100 Loss 0.5681 Accuracy 0.8285\n",
      "Epoch 36 Batch 150 Loss 0.5481 Accuracy 0.8338\n",
      "Epoch 36 Batch 200 Loss 0.5348 Accuracy 0.8385\n",
      "Epoch 36 Batch 250 Loss 0.5237 Accuracy 0.8425\n",
      "Epoch 36 Batch 300 Loss 0.5182 Accuracy 0.8445\n",
      "Epoch 36 Batch 350 Loss 0.5176 Accuracy 0.8456\n",
      "Epoch 36 Batch 400 Loss 0.5193 Accuracy 0.8454\n",
      "Epoch 36 Batch 450 Loss 0.5217 Accuracy 0.8454\n",
      "Epoch 36 Batch 500 Loss 0.5255 Accuracy 0.8448\n",
      "Epoch 36 Loss 0.5272 Accuracy 0.8448\n",
      "Time taken for 1 epoch: 156.79965043067932 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.5974 Accuracy 0.8247\n",
      "Epoch 37 Batch 50 Loss 0.5441 Accuracy 0.8348\n",
      "Epoch 37 Batch 100 Loss 0.5336 Accuracy 0.8362\n",
      "Epoch 37 Batch 150 Loss 0.5267 Accuracy 0.8397\n",
      "Epoch 37 Batch 200 Loss 0.5139 Accuracy 0.8445\n",
      "Epoch 37 Batch 250 Loss 0.5060 Accuracy 0.8475\n",
      "Epoch 37 Batch 300 Loss 0.4988 Accuracy 0.8506\n",
      "Epoch 37 Batch 350 Loss 0.4962 Accuracy 0.8522\n",
      "Epoch 37 Batch 400 Loss 0.4973 Accuracy 0.8524\n",
      "Epoch 37 Batch 450 Loss 0.4985 Accuracy 0.8523\n",
      "Epoch 37 Batch 500 Loss 0.5018 Accuracy 0.8521\n",
      "Epoch 37 Loss 0.5029 Accuracy 0.8523\n",
      "Time taken for 1 epoch: 157.65807032585144 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.5645 Accuracy 0.8065\n",
      "Epoch 38 Batch 50 Loss 0.5191 Accuracy 0.8394\n",
      "Epoch 38 Batch 100 Loss 0.5158 Accuracy 0.8428\n",
      "Epoch 38 Batch 150 Loss 0.5115 Accuracy 0.8453\n",
      "Epoch 38 Batch 200 Loss 0.5013 Accuracy 0.8489\n",
      "Epoch 38 Batch 250 Loss 0.4924 Accuracy 0.8524\n",
      "Epoch 38 Batch 300 Loss 0.4864 Accuracy 0.8543\n",
      "Epoch 38 Batch 350 Loss 0.4838 Accuracy 0.8560\n",
      "Epoch 38 Batch 400 Loss 0.4837 Accuracy 0.8566\n",
      "Epoch 38 Batch 450 Loss 0.4847 Accuracy 0.8569\n",
      "Epoch 38 Batch 500 Loss 0.4864 Accuracy 0.8567\n",
      "Epoch 38 Loss 0.4882 Accuracy 0.8563\n",
      "Time taken for 1 epoch: 157.0193510055542 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.5370 Accuracy 0.8505\n",
      "Epoch 39 Batch 50 Loss 0.5081 Accuracy 0.8464\n",
      "Epoch 39 Batch 100 Loss 0.4982 Accuracy 0.8483\n",
      "Epoch 39 Batch 150 Loss 0.4897 Accuracy 0.8519\n",
      "Epoch 39 Batch 200 Loss 0.4766 Accuracy 0.8563\n",
      "Epoch 39 Batch 250 Loss 0.4651 Accuracy 0.8605\n",
      "Epoch 39 Batch 300 Loss 0.4630 Accuracy 0.8620\n",
      "Epoch 39 Batch 350 Loss 0.4633 Accuracy 0.8624\n",
      "Epoch 39 Batch 400 Loss 0.4633 Accuracy 0.8630\n",
      "Epoch 39 Batch 450 Loss 0.4657 Accuracy 0.8628\n",
      "Epoch 39 Batch 500 Loss 0.4677 Accuracy 0.8623\n",
      "Epoch 39 Loss 0.4680 Accuracy 0.8624\n",
      "Time taken for 1 epoch: 158.27234840393066 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.5089 Accuracy 0.8502\n",
      "Epoch 40 Batch 50 Loss 0.4900 Accuracy 0.8491\n",
      "Epoch 40 Batch 100 Loss 0.4879 Accuracy 0.8499\n",
      "Epoch 40 Batch 150 Loss 0.4756 Accuracy 0.8553\n",
      "Epoch 40 Batch 200 Loss 0.4607 Accuracy 0.8604\n",
      "Epoch 40 Batch 250 Loss 0.4539 Accuracy 0.8635\n",
      "Epoch 40 Batch 300 Loss 0.4500 Accuracy 0.8649\n",
      "Epoch 40 Batch 350 Loss 0.4471 Accuracy 0.8661\n",
      "Epoch 40 Batch 400 Loss 0.4491 Accuracy 0.8660\n",
      "Epoch 40 Batch 450 Loss 0.4521 Accuracy 0.8656\n",
      "Epoch 40 Batch 500 Loss 0.4551 Accuracy 0.8650\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train\\ckpt-8\n",
      "Epoch 40 Loss 0.4551 Accuracy 0.8649\n",
      "Time taken for 1 epoch: 163.5844864845276 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.4360 Accuracy 0.8609\n",
      "Epoch 41 Batch 50 Loss 0.4453 Accuracy 0.8623\n",
      "Epoch 41 Batch 100 Loss 0.4546 Accuracy 0.8611\n",
      "Epoch 41 Batch 150 Loss 0.4435 Accuracy 0.8644\n",
      "Epoch 41 Batch 200 Loss 0.4334 Accuracy 0.8684\n",
      "Epoch 41 Batch 250 Loss 0.4247 Accuracy 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 300 Loss 0.4230 Accuracy 0.8727\n",
      "Epoch 41 Batch 350 Loss 0.4249 Accuracy 0.8722\n",
      "Epoch 41 Batch 400 Loss 0.4254 Accuracy 0.8724\n",
      "Epoch 41 Batch 450 Loss 0.4282 Accuracy 0.8718\n",
      "Epoch 41 Batch 500 Loss 0.4313 Accuracy 0.8712\n",
      "Epoch 41 Loss 0.4336 Accuracy 0.8707\n",
      "Time taken for 1 epoch: 154.3997871875763 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.4013 Accuracy 0.8794\n",
      "Epoch 42 Batch 50 Loss 0.4547 Accuracy 0.8611\n",
      "Epoch 42 Batch 100 Loss 0.4531 Accuracy 0.8619\n",
      "Epoch 42 Batch 150 Loss 0.4358 Accuracy 0.8674\n",
      "Epoch 42 Batch 200 Loss 0.4257 Accuracy 0.8709\n",
      "Epoch 42 Batch 250 Loss 0.4161 Accuracy 0.8746\n",
      "Epoch 42 Batch 300 Loss 0.4129 Accuracy 0.8758\n",
      "Epoch 42 Batch 350 Loss 0.4134 Accuracy 0.8758\n",
      "Epoch 42 Batch 400 Loss 0.4165 Accuracy 0.8754\n",
      "Epoch 42 Batch 450 Loss 0.4175 Accuracy 0.8753\n",
      "Epoch 42 Batch 500 Loss 0.4217 Accuracy 0.8743\n",
      "Epoch 42 Loss 0.4228 Accuracy 0.8740\n",
      "Time taken for 1 epoch: 158.03592705726624 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.4278 Accuracy 0.8495\n",
      "Epoch 43 Batch 50 Loss 0.4399 Accuracy 0.8616\n",
      "Epoch 43 Batch 100 Loss 0.4387 Accuracy 0.8634\n",
      "Epoch 43 Batch 150 Loss 0.4303 Accuracy 0.8674\n",
      "Epoch 43 Batch 200 Loss 0.4209 Accuracy 0.8714\n",
      "Epoch 43 Batch 250 Loss 0.4109 Accuracy 0.8750\n",
      "Epoch 43 Batch 300 Loss 0.4083 Accuracy 0.8766\n",
      "Epoch 43 Batch 350 Loss 0.4083 Accuracy 0.8768\n",
      "Epoch 43 Batch 400 Loss 0.4088 Accuracy 0.8771\n",
      "Epoch 43 Batch 450 Loss 0.4112 Accuracy 0.8769\n",
      "Epoch 43 Batch 500 Loss 0.4137 Accuracy 0.8768\n",
      "Epoch 43 Loss 0.4152 Accuracy 0.8764\n",
      "Time taken for 1 epoch: 157.09019351005554 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.4714 Accuracy 0.8538\n",
      "Epoch 44 Batch 50 Loss 0.4330 Accuracy 0.8652\n",
      "Epoch 44 Batch 100 Loss 0.4219 Accuracy 0.8692\n",
      "Epoch 44 Batch 150 Loss 0.4095 Accuracy 0.8734\n",
      "Epoch 44 Batch 200 Loss 0.4036 Accuracy 0.8759\n",
      "Epoch 44 Batch 250 Loss 0.3966 Accuracy 0.8791\n",
      "Epoch 44 Batch 300 Loss 0.3945 Accuracy 0.8808\n",
      "Epoch 44 Batch 350 Loss 0.3937 Accuracy 0.8815\n",
      "Epoch 44 Batch 400 Loss 0.3938 Accuracy 0.8819\n",
      "Epoch 44 Batch 450 Loss 0.3964 Accuracy 0.8814\n",
      "Epoch 44 Batch 500 Loss 0.3981 Accuracy 0.8812\n",
      "Epoch 44 Loss 0.3994 Accuracy 0.8807\n",
      "Time taken for 1 epoch: 157.79369449615479 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.4710 Accuracy 0.8270\n",
      "Epoch 45 Batch 50 Loss 0.4133 Accuracy 0.8691\n",
      "Epoch 45 Batch 100 Loss 0.4104 Accuracy 0.8724\n",
      "Epoch 45 Batch 150 Loss 0.4057 Accuracy 0.8747\n",
      "Epoch 45 Batch 200 Loss 0.3951 Accuracy 0.8782\n",
      "Epoch 45 Batch 250 Loss 0.3895 Accuracy 0.8810\n",
      "Epoch 45 Batch 300 Loss 0.3863 Accuracy 0.8825\n",
      "Epoch 45 Batch 350 Loss 0.3855 Accuracy 0.8832\n",
      "Epoch 45 Batch 400 Loss 0.3863 Accuracy 0.8834\n",
      "Epoch 45 Batch 450 Loss 0.3871 Accuracy 0.8832\n",
      "Epoch 45 Batch 500 Loss 0.3907 Accuracy 0.8827\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train\\ckpt-9\n",
      "Epoch 45 Loss 0.3911 Accuracy 0.8828\n",
      "Time taken for 1 epoch: 163.84850096702576 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.3992 Accuracy 0.8790\n",
      "Epoch 46 Batch 50 Loss 0.3997 Accuracy 0.8764\n",
      "Epoch 46 Batch 100 Loss 0.4008 Accuracy 0.8775\n",
      "Epoch 46 Batch 150 Loss 0.3879 Accuracy 0.8814\n",
      "Epoch 46 Batch 200 Loss 0.3814 Accuracy 0.8835\n",
      "Epoch 46 Batch 250 Loss 0.3746 Accuracy 0.8860\n",
      "Epoch 46 Batch 300 Loss 0.3698 Accuracy 0.8878\n",
      "Epoch 46 Batch 350 Loss 0.3702 Accuracy 0.8884\n",
      "Epoch 46 Batch 400 Loss 0.3713 Accuracy 0.8883\n",
      "Epoch 46 Batch 450 Loss 0.3733 Accuracy 0.8879\n",
      "Epoch 46 Batch 500 Loss 0.3768 Accuracy 0.8873\n",
      "Epoch 46 Loss 0.3791 Accuracy 0.8868\n",
      "Time taken for 1 epoch: 158.86055994033813 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.4371 Accuracy 0.8601\n",
      "Epoch 47 Batch 50 Loss 0.3862 Accuracy 0.8818\n",
      "Epoch 47 Batch 100 Loss 0.3958 Accuracy 0.8782\n",
      "Epoch 47 Batch 150 Loss 0.3832 Accuracy 0.8818\n",
      "Epoch 47 Batch 200 Loss 0.3738 Accuracy 0.8852\n",
      "Epoch 47 Batch 250 Loss 0.3663 Accuracy 0.8884\n",
      "Epoch 47 Batch 300 Loss 0.3629 Accuracy 0.8904\n",
      "Epoch 47 Batch 350 Loss 0.3631 Accuracy 0.8909\n",
      "Epoch 47 Batch 400 Loss 0.3647 Accuracy 0.8906\n",
      "Epoch 47 Batch 450 Loss 0.3675 Accuracy 0.8897\n",
      "Epoch 47 Batch 500 Loss 0.3686 Accuracy 0.8898\n",
      "Epoch 47 Loss 0.3699 Accuracy 0.8896\n",
      "Time taken for 1 epoch: 157.21453404426575 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.4776 Accuracy 0.8462\n",
      "Epoch 48 Batch 50 Loss 0.3872 Accuracy 0.8805\n",
      "Epoch 48 Batch 100 Loss 0.3851 Accuracy 0.8813\n",
      "Epoch 48 Batch 150 Loss 0.3731 Accuracy 0.8853\n",
      "Epoch 48 Batch 200 Loss 0.3652 Accuracy 0.8886\n",
      "Epoch 48 Batch 250 Loss 0.3593 Accuracy 0.8909\n",
      "Epoch 48 Batch 300 Loss 0.3573 Accuracy 0.8922\n",
      "Epoch 48 Batch 350 Loss 0.3564 Accuracy 0.8925\n",
      "Epoch 48 Batch 400 Loss 0.3558 Accuracy 0.8927\n",
      "Epoch 48 Batch 450 Loss 0.3565 Accuracy 0.8926\n",
      "Epoch 48 Batch 500 Loss 0.3587 Accuracy 0.8923\n",
      "Epoch 48 Loss 0.3590 Accuracy 0.8923\n",
      "Time taken for 1 epoch: 155.24515438079834 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.3832 Accuracy 0.8926\n",
      "Epoch 49 Batch 50 Loss 0.3820 Accuracy 0.8806\n",
      "Epoch 49 Batch 100 Loss 0.3733 Accuracy 0.8842\n",
      "Epoch 49 Batch 150 Loss 0.3653 Accuracy 0.8870\n",
      "Epoch 49 Batch 200 Loss 0.3544 Accuracy 0.8907\n",
      "Epoch 49 Batch 250 Loss 0.3487 Accuracy 0.8928\n",
      "Epoch 49 Batch 300 Loss 0.3458 Accuracy 0.8944\n",
      "Epoch 49 Batch 350 Loss 0.3441 Accuracy 0.8954\n",
      "Epoch 49 Batch 400 Loss 0.3454 Accuracy 0.8955\n",
      "Epoch 49 Batch 450 Loss 0.3460 Accuracy 0.8956\n",
      "Epoch 49 Batch 500 Loss 0.3492 Accuracy 0.8951\n",
      "Epoch 49 Loss 0.3494 Accuracy 0.8950\n",
      "Time taken for 1 epoch: 155.10029482841492 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.3837 Accuracy 0.8915\n",
      "Epoch 50 Batch 50 Loss 0.3669 Accuracy 0.8870\n",
      "Epoch 50 Batch 100 Loss 0.3600 Accuracy 0.8884\n",
      "Epoch 50 Batch 150 Loss 0.3509 Accuracy 0.8921\n",
      "Epoch 50 Batch 200 Loss 0.3384 Accuracy 0.8964\n",
      "Epoch 50 Batch 250 Loss 0.3350 Accuracy 0.8976\n",
      "Epoch 50 Batch 300 Loss 0.3333 Accuracy 0.8986\n",
      "Epoch 50 Batch 350 Loss 0.3327 Accuracy 0.8992\n",
      "Epoch 50 Batch 400 Loss 0.3338 Accuracy 0.8989\n",
      "Epoch 50 Batch 450 Loss 0.3372 Accuracy 0.8984\n",
      "Epoch 50 Batch 500 Loss 0.3393 Accuracy 0.8980\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train\\ckpt-10\n",
      "Epoch 50 Loss 0.3397 Accuracy 0.8978\n",
      "Time taken for 1 epoch: 162.9537971019745 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.3076 Accuracy 0.8969\n",
      "Epoch 51 Batch 50 Loss 0.3582 Accuracy 0.8913\n",
      "Epoch 51 Batch 100 Loss 0.3553 Accuracy 0.8915\n",
      "Epoch 51 Batch 150 Loss 0.3479 Accuracy 0.8945\n",
      "Epoch 51 Batch 200 Loss 0.3382 Accuracy 0.8970\n",
      "Epoch 51 Batch 250 Loss 0.3302 Accuracy 0.9001\n",
      "Epoch 51 Batch 300 Loss 0.3276 Accuracy 0.9009\n",
      "Epoch 51 Batch 350 Loss 0.3260 Accuracy 0.9021\n",
      "Epoch 51 Batch 400 Loss 0.3282 Accuracy 0.9017\n",
      "Epoch 51 Batch 450 Loss 0.3286 Accuracy 0.9017\n",
      "Epoch 51 Batch 500 Loss 0.3309 Accuracy 0.9011\n",
      "Epoch 51 Loss 0.3318 Accuracy 0.9010\n",
      "Time taken for 1 epoch: 156.32001638412476 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.3431 Accuracy 0.8969\n",
      "Epoch 52 Batch 50 Loss 0.3529 Accuracy 0.8947\n",
      "Epoch 52 Batch 100 Loss 0.3432 Accuracy 0.8950\n",
      "Epoch 52 Batch 150 Loss 0.3361 Accuracy 0.8985\n",
      "Epoch 52 Batch 200 Loss 0.3303 Accuracy 0.9002\n",
      "Epoch 52 Batch 250 Loss 0.3226 Accuracy 0.9029\n",
      "Epoch 52 Batch 300 Loss 0.3172 Accuracy 0.9048\n",
      "Epoch 52 Batch 350 Loss 0.3157 Accuracy 0.9055\n",
      "Epoch 52 Batch 400 Loss 0.3184 Accuracy 0.9051\n",
      "Epoch 52 Batch 450 Loss 0.3207 Accuracy 0.9048\n",
      "Epoch 52 Batch 500 Loss 0.3219 Accuracy 0.9043\n",
      "Epoch 52 Loss 0.3230 Accuracy 0.9040\n",
      "Time taken for 1 epoch: 156.00065994262695 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.3050 Accuracy 0.9135\n",
      "Epoch 53 Batch 50 Loss 0.3423 Accuracy 0.8959\n",
      "Epoch 53 Batch 100 Loss 0.3350 Accuracy 0.8980\n",
      "Epoch 53 Batch 150 Loss 0.3267 Accuracy 0.9007\n",
      "Epoch 53 Batch 200 Loss 0.3200 Accuracy 0.9025\n",
      "Epoch 53 Batch 250 Loss 0.3148 Accuracy 0.9045\n",
      "Epoch 53 Batch 300 Loss 0.3104 Accuracy 0.9057\n",
      "Epoch 53 Batch 350 Loss 0.3089 Accuracy 0.9061\n",
      "Epoch 53 Batch 400 Loss 0.3092 Accuracy 0.9064\n",
      "Epoch 53 Batch 450 Loss 0.3121 Accuracy 0.9059\n",
      "Epoch 53 Batch 500 Loss 0.3129 Accuracy 0.9056\n",
      "Epoch 53 Loss 0.3149 Accuracy 0.9052\n",
      "Time taken for 1 epoch: 156.12841200828552 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.3809 Accuracy 0.8850\n",
      "Epoch 54 Batch 50 Loss 0.3294 Accuracy 0.8991\n",
      "Epoch 54 Batch 100 Loss 0.3226 Accuracy 0.9022\n",
      "Epoch 54 Batch 150 Loss 0.3219 Accuracy 0.9025\n",
      "Epoch 54 Batch 200 Loss 0.3156 Accuracy 0.9050\n",
      "Epoch 54 Batch 250 Loss 0.3093 Accuracy 0.9065\n",
      "Epoch 54 Batch 300 Loss 0.3090 Accuracy 0.9071\n",
      "Epoch 54 Batch 350 Loss 0.3054 Accuracy 0.9080\n",
      "Epoch 54 Batch 400 Loss 0.3047 Accuracy 0.9086\n",
      "Epoch 54 Batch 450 Loss 0.3059 Accuracy 0.9088\n",
      "Epoch 54 Batch 500 Loss 0.3092 Accuracy 0.9079\n",
      "Epoch 54 Loss 0.3100 Accuracy 0.9078\n",
      "Time taken for 1 epoch: 156.93468189239502 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 0 Loss 0.2067 Accuracy 0.9419\n",
      "Epoch 55 Batch 50 Loss 0.3174 Accuracy 0.9040\n",
      "Epoch 55 Batch 100 Loss 0.3221 Accuracy 0.9025\n",
      "Epoch 55 Batch 150 Loss 0.3113 Accuracy 0.9050\n",
      "Epoch 55 Batch 200 Loss 0.3031 Accuracy 0.9078\n",
      "Epoch 55 Batch 250 Loss 0.2950 Accuracy 0.9100\n",
      "Epoch 55 Batch 300 Loss 0.2926 Accuracy 0.9107\n",
      "Epoch 55 Batch 350 Loss 0.2924 Accuracy 0.9111\n",
      "Epoch 55 Batch 400 Loss 0.2928 Accuracy 0.9114\n",
      "Epoch 55 Batch 450 Loss 0.2958 Accuracy 0.9109\n",
      "Epoch 55 Batch 500 Loss 0.2984 Accuracy 0.9104\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train\\ckpt-11\n",
      "Epoch 55 Loss 0.2997 Accuracy 0.9103\n",
      "Time taken for 1 epoch: 163.48155522346497 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.4259 Accuracy 0.8706\n",
      "Epoch 56 Batch 50 Loss 0.3093 Accuracy 0.9040\n",
      "Epoch 56 Batch 100 Loss 0.3098 Accuracy 0.9040\n",
      "Epoch 56 Batch 150 Loss 0.3052 Accuracy 0.9062\n",
      "Epoch 56 Batch 200 Loss 0.2965 Accuracy 0.9089\n",
      "Epoch 56 Batch 250 Loss 0.2915 Accuracy 0.9110\n",
      "Epoch 56 Batch 300 Loss 0.2902 Accuracy 0.9121\n",
      "Epoch 56 Batch 350 Loss 0.2918 Accuracy 0.9117\n",
      "Epoch 56 Batch 400 Loss 0.2921 Accuracy 0.9118\n",
      "Epoch 56 Batch 450 Loss 0.2927 Accuracy 0.9119\n",
      "Epoch 56 Batch 500 Loss 0.2931 Accuracy 0.9119\n",
      "Epoch 56 Loss 0.2938 Accuracy 0.9118\n",
      "Time taken for 1 epoch: 156.92057490348816 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.2488 Accuracy 0.9456\n",
      "Epoch 57 Batch 50 Loss 0.3100 Accuracy 0.9067\n",
      "Epoch 57 Batch 100 Loss 0.3120 Accuracy 0.9052\n",
      "Epoch 57 Batch 150 Loss 0.2995 Accuracy 0.9095\n",
      "Epoch 57 Batch 200 Loss 0.2917 Accuracy 0.9122\n",
      "Epoch 57 Batch 250 Loss 0.2861 Accuracy 0.9141\n",
      "Epoch 57 Batch 300 Loss 0.2828 Accuracy 0.9154\n",
      "Epoch 57 Batch 350 Loss 0.2821 Accuracy 0.9157\n",
      "Epoch 57 Batch 400 Loss 0.2822 Accuracy 0.9160\n",
      "Epoch 57 Batch 450 Loss 0.2847 Accuracy 0.9153\n",
      "Epoch 57 Batch 500 Loss 0.2862 Accuracy 0.9150\n",
      "Epoch 57 Loss 0.2873 Accuracy 0.9150\n",
      "Time taken for 1 epoch: 155.6428303718567 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.2613 Accuracy 0.9199\n",
      "Epoch 58 Batch 50 Loss 0.3011 Accuracy 0.9084\n",
      "Epoch 58 Batch 100 Loss 0.2971 Accuracy 0.9095\n",
      "Epoch 58 Batch 150 Loss 0.2883 Accuracy 0.9121\n",
      "Epoch 58 Batch 200 Loss 0.2803 Accuracy 0.9145\n",
      "Epoch 58 Batch 250 Loss 0.2731 Accuracy 0.9172\n",
      "Epoch 58 Batch 300 Loss 0.2720 Accuracy 0.9178\n",
      "Epoch 58 Batch 350 Loss 0.2738 Accuracy 0.9179\n",
      "Epoch 58 Batch 400 Loss 0.2751 Accuracy 0.9180\n",
      "Epoch 58 Batch 450 Loss 0.2755 Accuracy 0.9177\n",
      "Epoch 58 Batch 500 Loss 0.2780 Accuracy 0.9172\n",
      "Epoch 58 Loss 0.2796 Accuracy 0.9167\n",
      "Time taken for 1 epoch: 193.72893691062927 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.3355 Accuracy 0.8974\n",
      "Epoch 59 Batch 50 Loss 0.2989 Accuracy 0.9075\n",
      "Epoch 59 Batch 100 Loss 0.2948 Accuracy 0.9094\n",
      "Epoch 59 Batch 150 Loss 0.2903 Accuracy 0.9110\n",
      "Epoch 59 Batch 200 Loss 0.2818 Accuracy 0.9142\n",
      "Epoch 59 Batch 250 Loss 0.2744 Accuracy 0.9174\n",
      "Epoch 59 Batch 300 Loss 0.2736 Accuracy 0.9180\n",
      "Epoch 59 Batch 350 Loss 0.2714 Accuracy 0.9186\n",
      "Epoch 59 Batch 400 Loss 0.2724 Accuracy 0.9184\n",
      "Epoch 59 Batch 450 Loss 0.2744 Accuracy 0.9178\n",
      "Epoch 59 Batch 500 Loss 0.2756 Accuracy 0.9175\n",
      "Epoch 59 Loss 0.2764 Accuracy 0.9174\n",
      "Time taken for 1 epoch: 229.5419454574585 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.2171 Accuracy 0.9276\n",
      "Epoch 60 Batch 50 Loss 0.2837 Accuracy 0.9127\n",
      "Epoch 60 Batch 100 Loss 0.2833 Accuracy 0.9143\n",
      "Epoch 60 Batch 150 Loss 0.2751 Accuracy 0.9171\n",
      "Epoch 60 Batch 200 Loss 0.2700 Accuracy 0.9182\n",
      "Epoch 60 Batch 250 Loss 0.2671 Accuracy 0.9198\n",
      "Epoch 60 Batch 300 Loss 0.2647 Accuracy 0.9205\n",
      "Epoch 60 Batch 350 Loss 0.2658 Accuracy 0.9204\n",
      "Epoch 60 Batch 400 Loss 0.2646 Accuracy 0.9212\n",
      "Epoch 60 Batch 450 Loss 0.2659 Accuracy 0.9210\n",
      "Epoch 60 Batch 500 Loss 0.2676 Accuracy 0.9205\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train\\ckpt-12\n",
      "Epoch 60 Loss 0.2690 Accuracy 0.9202\n",
      "Time taken for 1 epoch: 164.00901770591736 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.2666 Accuracy 0.9028\n",
      "Epoch 61 Batch 50 Loss 0.2827 Accuracy 0.9129\n",
      "Epoch 61 Batch 100 Loss 0.2829 Accuracy 0.9144\n",
      "Epoch 61 Batch 150 Loss 0.2783 Accuracy 0.9158\n",
      "Epoch 61 Batch 200 Loss 0.2697 Accuracy 0.9187\n",
      "Epoch 61 Batch 250 Loss 0.2648 Accuracy 0.9208\n",
      "Epoch 61 Batch 300 Loss 0.2621 Accuracy 0.9217\n",
      "Epoch 61 Batch 350 Loss 0.2588 Accuracy 0.9228\n",
      "Epoch 61 Batch 400 Loss 0.2588 Accuracy 0.9229\n",
      "Epoch 61 Batch 450 Loss 0.2591 Accuracy 0.9231\n",
      "Epoch 61 Batch 500 Loss 0.2609 Accuracy 0.9228\n",
      "Epoch 61 Loss 0.2626 Accuracy 0.9224\n",
      "Time taken for 1 epoch: 156.94112420082092 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.2670 Accuracy 0.9249\n",
      "Epoch 62 Batch 50 Loss 0.2765 Accuracy 0.9146\n",
      "Epoch 62 Batch 100 Loss 0.2788 Accuracy 0.9145\n",
      "Epoch 62 Batch 150 Loss 0.2718 Accuracy 0.9163\n",
      "Epoch 62 Batch 200 Loss 0.2626 Accuracy 0.9201\n",
      "Epoch 62 Batch 250 Loss 0.2571 Accuracy 0.9220\n",
      "Epoch 62 Batch 300 Loss 0.2532 Accuracy 0.9236\n",
      "Epoch 62 Batch 350 Loss 0.2526 Accuracy 0.9242\n",
      "Epoch 62 Batch 400 Loss 0.2537 Accuracy 0.9243\n",
      "Epoch 62 Batch 450 Loss 0.2541 Accuracy 0.9245\n",
      "Epoch 62 Batch 500 Loss 0.2558 Accuracy 0.9242\n",
      "Epoch 62 Loss 0.2569 Accuracy 0.9240\n",
      "Time taken for 1 epoch: 156.95690083503723 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.3859 Accuracy 0.8655\n",
      "Epoch 63 Batch 50 Loss 0.2747 Accuracy 0.9142\n",
      "Epoch 63 Batch 100 Loss 0.2665 Accuracy 0.9184\n",
      "Epoch 63 Batch 150 Loss 0.2608 Accuracy 0.9211\n",
      "Epoch 63 Batch 200 Loss 0.2540 Accuracy 0.9234\n",
      "Epoch 63 Batch 250 Loss 0.2514 Accuracy 0.9249\n",
      "Epoch 63 Batch 300 Loss 0.2489 Accuracy 0.9260\n",
      "Epoch 63 Batch 350 Loss 0.2470 Accuracy 0.9269\n",
      "Epoch 63 Batch 400 Loss 0.2479 Accuracy 0.9269\n",
      "Epoch 63 Batch 450 Loss 0.2491 Accuracy 0.9269\n",
      "Epoch 63 Batch 500 Loss 0.2520 Accuracy 0.9262\n",
      "Epoch 63 Loss 0.2531 Accuracy 0.9258\n",
      "Time taken for 1 epoch: 157.11592650413513 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.2138 Accuracy 0.9486\n",
      "Epoch 64 Batch 50 Loss 0.2656 Accuracy 0.9209\n",
      "Epoch 64 Batch 100 Loss 0.2687 Accuracy 0.9199\n",
      "Epoch 64 Batch 150 Loss 0.2578 Accuracy 0.9223\n",
      "Epoch 64 Batch 200 Loss 0.2519 Accuracy 0.9249\n",
      "Epoch 64 Batch 250 Loss 0.2455 Accuracy 0.9266\n",
      "Epoch 64 Batch 300 Loss 0.2431 Accuracy 0.9276\n",
      "Epoch 64 Batch 350 Loss 0.2430 Accuracy 0.9278\n",
      "Epoch 64 Batch 400 Loss 0.2438 Accuracy 0.9278\n",
      "Epoch 64 Batch 450 Loss 0.2473 Accuracy 0.9268\n",
      "Epoch 64 Batch 500 Loss 0.2492 Accuracy 0.9265\n",
      "Epoch 64 Loss 0.2503 Accuracy 0.9263\n",
      "Time taken for 1 epoch: 155.52654576301575 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.2144 Accuracy 0.9474\n",
      "Epoch 65 Batch 50 Loss 0.2535 Accuracy 0.9252\n",
      "Epoch 65 Batch 100 Loss 0.2550 Accuracy 0.9245\n",
      "Epoch 65 Batch 150 Loss 0.2504 Accuracy 0.9259\n",
      "Epoch 65 Batch 200 Loss 0.2447 Accuracy 0.9275\n",
      "Epoch 65 Batch 250 Loss 0.2421 Accuracy 0.9280\n",
      "Epoch 65 Batch 300 Loss 0.2416 Accuracy 0.9283\n",
      "Epoch 65 Batch 350 Loss 0.2396 Accuracy 0.9289\n",
      "Epoch 65 Batch 400 Loss 0.2397 Accuracy 0.9290\n",
      "Epoch 65 Batch 450 Loss 0.2400 Accuracy 0.9290\n",
      "Epoch 65 Batch 500 Loss 0.2423 Accuracy 0.9286\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train\\ckpt-13\n",
      "Epoch 65 Loss 0.2427 Accuracy 0.9287\n",
      "Time taken for 1 epoch: 164.7392463684082 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.1814 Accuracy 0.9351\n",
      "Epoch 66 Batch 50 Loss 0.2440 Accuracy 0.9250\n",
      "Epoch 66 Batch 100 Loss 0.2473 Accuracy 0.9240\n",
      "Epoch 66 Batch 150 Loss 0.2468 Accuracy 0.9255\n",
      "Epoch 66 Batch 200 Loss 0.2385 Accuracy 0.9285\n",
      "Epoch 66 Batch 250 Loss 0.2338 Accuracy 0.9305\n",
      "Epoch 66 Batch 300 Loss 0.2344 Accuracy 0.9306\n",
      "Epoch 66 Batch 350 Loss 0.2338 Accuracy 0.9311\n",
      "Epoch 66 Batch 400 Loss 0.2352 Accuracy 0.9309\n",
      "Epoch 66 Batch 450 Loss 0.2363 Accuracy 0.9309\n",
      "Epoch 66 Batch 500 Loss 0.2382 Accuracy 0.9303\n",
      "Epoch 66 Loss 0.2386 Accuracy 0.9303\n",
      "Time taken for 1 epoch: 164.54469966888428 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.2653 Accuracy 0.9126\n",
      "Epoch 67 Batch 50 Loss 0.2550 Accuracy 0.9227\n",
      "Epoch 67 Batch 100 Loss 0.2539 Accuracy 0.9230\n",
      "Epoch 67 Batch 150 Loss 0.2456 Accuracy 0.9259\n",
      "Epoch 67 Batch 200 Loss 0.2386 Accuracy 0.9283\n",
      "Epoch 67 Batch 250 Loss 0.2338 Accuracy 0.9296\n",
      "Epoch 67 Batch 300 Loss 0.2301 Accuracy 0.9311\n",
      "Epoch 67 Batch 350 Loss 0.2301 Accuracy 0.9313\n",
      "Epoch 67 Batch 400 Loss 0.2315 Accuracy 0.9311\n",
      "Epoch 67 Batch 450 Loss 0.2320 Accuracy 0.9313\n",
      "Epoch 67 Batch 500 Loss 0.2332 Accuracy 0.9311\n",
      "Epoch 67 Loss 0.2344 Accuracy 0.9308\n",
      "Time taken for 1 epoch: 161.90804243087769 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.2254 Accuracy 0.9254\n",
      "Epoch 68 Batch 50 Loss 0.2406 Accuracy 0.9270\n",
      "Epoch 68 Batch 100 Loss 0.2427 Accuracy 0.9258\n",
      "Epoch 68 Batch 150 Loss 0.2417 Accuracy 0.9260\n",
      "Epoch 68 Batch 200 Loss 0.2334 Accuracy 0.9291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 250 Loss 0.2304 Accuracy 0.9304\n",
      "Epoch 68 Batch 300 Loss 0.2266 Accuracy 0.9318\n",
      "Epoch 68 Batch 350 Loss 0.2269 Accuracy 0.9321\n",
      "Epoch 68 Batch 400 Loss 0.2293 Accuracy 0.9318\n",
      "Epoch 68 Batch 450 Loss 0.2301 Accuracy 0.9317\n",
      "Epoch 68 Batch 500 Loss 0.2321 Accuracy 0.9314\n",
      "Epoch 68 Loss 0.2325 Accuracy 0.9314\n",
      "Time taken for 1 epoch: 166.60879826545715 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.2599 Accuracy 0.9226\n",
      "Epoch 69 Batch 50 Loss 0.2414 Accuracy 0.9286\n",
      "Epoch 69 Batch 100 Loss 0.2362 Accuracy 0.9286\n",
      "Epoch 69 Batch 150 Loss 0.2329 Accuracy 0.9295\n",
      "Epoch 69 Batch 200 Loss 0.2270 Accuracy 0.9317\n",
      "Epoch 69 Batch 250 Loss 0.2220 Accuracy 0.9338\n",
      "Epoch 69 Batch 300 Loss 0.2198 Accuracy 0.9348\n",
      "Epoch 69 Batch 350 Loss 0.2197 Accuracy 0.9347\n",
      "Epoch 69 Batch 400 Loss 0.2204 Accuracy 0.9345\n",
      "Epoch 69 Batch 450 Loss 0.2221 Accuracy 0.9343\n",
      "Epoch 69 Batch 500 Loss 0.2247 Accuracy 0.9336\n",
      "Epoch 69 Loss 0.2250 Accuracy 0.9336\n",
      "Time taken for 1 epoch: 158.0664038658142 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.2538 Accuracy 0.9085\n",
      "Epoch 70 Batch 50 Loss 0.2421 Accuracy 0.9265\n",
      "Epoch 70 Batch 100 Loss 0.2420 Accuracy 0.9266\n",
      "Epoch 70 Batch 150 Loss 0.2330 Accuracy 0.9291\n",
      "Epoch 70 Batch 200 Loss 0.2260 Accuracy 0.9315\n",
      "Epoch 70 Batch 250 Loss 0.2221 Accuracy 0.9331\n",
      "Epoch 70 Batch 300 Loss 0.2205 Accuracy 0.9341\n",
      "Epoch 70 Batch 350 Loss 0.2201 Accuracy 0.9344\n",
      "Epoch 70 Batch 400 Loss 0.2194 Accuracy 0.9349\n",
      "Epoch 70 Batch 450 Loss 0.2203 Accuracy 0.9348\n",
      "Epoch 70 Batch 500 Loss 0.2218 Accuracy 0.9346\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train\\ckpt-14\n",
      "Epoch 70 Loss 0.2218 Accuracy 0.9346\n",
      "Time taken for 1 epoch: 162.6244821548462 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.3122 Accuracy 0.9094\n",
      "Epoch 71 Batch 50 Loss 0.2334 Accuracy 0.9288\n",
      "Epoch 71 Batch 100 Loss 0.2315 Accuracy 0.9293\n",
      "Epoch 71 Batch 150 Loss 0.2284 Accuracy 0.9308\n",
      "Epoch 71 Batch 200 Loss 0.2203 Accuracy 0.9338\n",
      "Epoch 71 Batch 250 Loss 0.2157 Accuracy 0.9352\n",
      "Epoch 71 Batch 300 Loss 0.2142 Accuracy 0.9361\n",
      "Epoch 71 Batch 350 Loss 0.2145 Accuracy 0.9364\n",
      "Epoch 71 Batch 400 Loss 0.2150 Accuracy 0.9365\n",
      "Epoch 71 Batch 450 Loss 0.2162 Accuracy 0.9362\n",
      "Epoch 71 Batch 500 Loss 0.2172 Accuracy 0.9359\n",
      "Epoch 71 Loss 0.2182 Accuracy 0.9357\n",
      "Time taken for 1 epoch: 154.73039054870605 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.2757 Accuracy 0.9225\n",
      "Epoch 72 Batch 50 Loss 0.2373 Accuracy 0.9262\n",
      "Epoch 72 Batch 100 Loss 0.2280 Accuracy 0.9300\n",
      "Epoch 72 Batch 150 Loss 0.2226 Accuracy 0.9320\n",
      "Epoch 72 Batch 200 Loss 0.2197 Accuracy 0.9333\n",
      "Epoch 72 Batch 250 Loss 0.2137 Accuracy 0.9359\n",
      "Epoch 72 Batch 300 Loss 0.2111 Accuracy 0.9367\n",
      "Epoch 72 Batch 350 Loss 0.2116 Accuracy 0.9368\n",
      "Epoch 72 Batch 400 Loss 0.2128 Accuracy 0.9368\n",
      "Epoch 72 Batch 450 Loss 0.2128 Accuracy 0.9370\n",
      "Epoch 72 Batch 500 Loss 0.2146 Accuracy 0.9367\n",
      "Epoch 72 Loss 0.2153 Accuracy 0.9366\n",
      "Time taken for 1 epoch: 155.88135290145874 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.2583 Accuracy 0.9322\n",
      "Epoch 73 Batch 50 Loss 0.2146 Accuracy 0.9355\n",
      "Epoch 73 Batch 100 Loss 0.2184 Accuracy 0.9350\n",
      "Epoch 73 Batch 150 Loss 0.2104 Accuracy 0.9375\n",
      "Epoch 73 Batch 200 Loss 0.2085 Accuracy 0.9379\n",
      "Epoch 73 Batch 250 Loss 0.2049 Accuracy 0.9393\n",
      "Epoch 73 Batch 300 Loss 0.2060 Accuracy 0.9392\n",
      "Epoch 73 Batch 350 Loss 0.2074 Accuracy 0.9388\n",
      "Epoch 73 Batch 400 Loss 0.2080 Accuracy 0.9387\n",
      "Epoch 73 Batch 450 Loss 0.2108 Accuracy 0.9379\n",
      "Epoch 73 Batch 500 Loss 0.2135 Accuracy 0.9374\n",
      "Epoch 73 Loss 0.2140 Accuracy 0.9372\n",
      "Time taken for 1 epoch: 155.51673913002014 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.2001 Accuracy 0.9349\n",
      "Epoch 74 Batch 50 Loss 0.2250 Accuracy 0.9311\n",
      "Epoch 74 Batch 100 Loss 0.2181 Accuracy 0.9341\n",
      "Epoch 74 Batch 150 Loss 0.2125 Accuracy 0.9353\n",
      "Epoch 74 Batch 200 Loss 0.2096 Accuracy 0.9368\n",
      "Epoch 74 Batch 250 Loss 0.2042 Accuracy 0.9388\n",
      "Epoch 74 Batch 300 Loss 0.2037 Accuracy 0.9395\n",
      "Epoch 74 Batch 350 Loss 0.2042 Accuracy 0.9397\n",
      "Epoch 74 Batch 400 Loss 0.2053 Accuracy 0.9397\n",
      "Epoch 74 Batch 450 Loss 0.2062 Accuracy 0.9396\n",
      "Epoch 74 Batch 500 Loss 0.2080 Accuracy 0.9393\n",
      "Epoch 74 Loss 0.2080 Accuracy 0.9393\n",
      "Time taken for 1 epoch: 155.53357553482056 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.2258 Accuracy 0.9310\n",
      "Epoch 75 Batch 50 Loss 0.2214 Accuracy 0.9331\n",
      "Epoch 75 Batch 100 Loss 0.2219 Accuracy 0.9332\n",
      "Epoch 75 Batch 150 Loss 0.2134 Accuracy 0.9367\n",
      "Epoch 75 Batch 200 Loss 0.2081 Accuracy 0.9389\n",
      "Epoch 75 Batch 250 Loss 0.2051 Accuracy 0.9398\n",
      "Epoch 75 Batch 300 Loss 0.2040 Accuracy 0.9401\n",
      "Epoch 75 Batch 350 Loss 0.2035 Accuracy 0.9405\n",
      "Epoch 75 Batch 400 Loss 0.2034 Accuracy 0.9407\n",
      "Epoch 75 Batch 450 Loss 0.2049 Accuracy 0.9404\n",
      "Epoch 75 Batch 500 Loss 0.2059 Accuracy 0.9402\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train\\ckpt-15\n",
      "Epoch 75 Loss 0.2068 Accuracy 0.9400\n",
      "Time taken for 1 epoch: 161.72545838356018 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.1775 Accuracy 0.9519\n",
      "Epoch 76 Batch 50 Loss 0.2113 Accuracy 0.9358\n",
      "Epoch 76 Batch 100 Loss 0.2159 Accuracy 0.9351\n",
      "Epoch 76 Batch 150 Loss 0.2082 Accuracy 0.9377\n",
      "Epoch 76 Batch 200 Loss 0.2015 Accuracy 0.9402\n",
      "Epoch 76 Batch 250 Loss 0.2012 Accuracy 0.9408\n",
      "Epoch 76 Batch 300 Loss 0.1997 Accuracy 0.9414\n",
      "Epoch 76 Batch 350 Loss 0.1994 Accuracy 0.9416\n",
      "Epoch 76 Batch 400 Loss 0.2020 Accuracy 0.9410\n",
      "Epoch 76 Batch 450 Loss 0.2023 Accuracy 0.9409\n",
      "Epoch 76 Batch 500 Loss 0.2045 Accuracy 0.9404\n",
      "Epoch 76 Loss 0.2054 Accuracy 0.9401\n",
      "Time taken for 1 epoch: 155.5259063243866 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.2735 Accuracy 0.9251\n",
      "Epoch 77 Batch 50 Loss 0.2009 Accuracy 0.9390\n",
      "Epoch 77 Batch 100 Loss 0.2041 Accuracy 0.9394\n",
      "Epoch 77 Batch 150 Loss 0.2016 Accuracy 0.9404\n",
      "Epoch 77 Batch 200 Loss 0.1980 Accuracy 0.9420\n",
      "Epoch 77 Batch 250 Loss 0.1949 Accuracy 0.9429\n",
      "Epoch 77 Batch 300 Loss 0.1940 Accuracy 0.9430\n",
      "Epoch 77 Batch 350 Loss 0.1943 Accuracy 0.9432\n",
      "Epoch 77 Batch 400 Loss 0.1954 Accuracy 0.9429\n",
      "Epoch 77 Batch 450 Loss 0.1975 Accuracy 0.9424\n",
      "Epoch 77 Batch 500 Loss 0.1992 Accuracy 0.9421\n",
      "Epoch 77 Loss 0.2005 Accuracy 0.9417\n",
      "Time taken for 1 epoch: 155.03362321853638 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.2457 Accuracy 0.9377\n",
      "Epoch 78 Batch 50 Loss 0.2067 Accuracy 0.9387\n",
      "Epoch 78 Batch 100 Loss 0.2089 Accuracy 0.9379\n",
      "Epoch 78 Batch 150 Loss 0.2074 Accuracy 0.9383\n",
      "Epoch 78 Batch 200 Loss 0.1993 Accuracy 0.9406\n",
      "Epoch 78 Batch 250 Loss 0.1959 Accuracy 0.9417\n",
      "Epoch 78 Batch 300 Loss 0.1939 Accuracy 0.9424\n",
      "Epoch 78 Batch 350 Loss 0.1938 Accuracy 0.9425\n",
      "Epoch 78 Batch 400 Loss 0.1929 Accuracy 0.9428\n",
      "Epoch 78 Batch 450 Loss 0.1942 Accuracy 0.9426\n",
      "Epoch 78 Batch 500 Loss 0.1957 Accuracy 0.9425\n",
      "Epoch 78 Loss 0.1960 Accuracy 0.9425\n",
      "Time taken for 1 epoch: 157.76144742965698 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.2935 Accuracy 0.9147\n",
      "Epoch 79 Batch 50 Loss 0.2076 Accuracy 0.9389\n",
      "Epoch 79 Batch 100 Loss 0.2122 Accuracy 0.9382\n",
      "Epoch 79 Batch 150 Loss 0.2057 Accuracy 0.9396\n",
      "Epoch 79 Batch 200 Loss 0.1972 Accuracy 0.9425\n",
      "Epoch 79 Batch 250 Loss 0.1934 Accuracy 0.9438\n",
      "Epoch 79 Batch 300 Loss 0.1916 Accuracy 0.9442\n",
      "Epoch 79 Batch 350 Loss 0.1914 Accuracy 0.9442\n",
      "Epoch 79 Batch 400 Loss 0.1926 Accuracy 0.9441\n",
      "Epoch 79 Batch 450 Loss 0.1933 Accuracy 0.9440\n",
      "Epoch 79 Batch 500 Loss 0.1936 Accuracy 0.9440\n",
      "Epoch 79 Loss 0.1940 Accuracy 0.9438\n",
      "Time taken for 1 epoch: 160.61764430999756 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.2474 Accuracy 0.9359\n",
      "Epoch 80 Batch 50 Loss 0.2009 Accuracy 0.9406\n",
      "Epoch 80 Batch 100 Loss 0.2024 Accuracy 0.9392\n",
      "Epoch 80 Batch 150 Loss 0.1969 Accuracy 0.9406\n",
      "Epoch 80 Batch 200 Loss 0.1913 Accuracy 0.9426\n",
      "Epoch 80 Batch 250 Loss 0.1879 Accuracy 0.9438\n",
      "Epoch 80 Batch 300 Loss 0.1874 Accuracy 0.9445\n",
      "Epoch 80 Batch 350 Loss 0.1866 Accuracy 0.9449\n",
      "Epoch 80 Batch 400 Loss 0.1880 Accuracy 0.9445\n",
      "Epoch 80 Batch 450 Loss 0.1888 Accuracy 0.9445\n",
      "Epoch 80 Batch 500 Loss 0.1907 Accuracy 0.9440\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train\\ckpt-16\n",
      "Epoch 80 Loss 0.1908 Accuracy 0.9440\n",
      "Time taken for 1 epoch: 166.18359375 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.2155 Accuracy 0.9497\n",
      "Epoch 81 Batch 50 Loss 0.2091 Accuracy 0.9380\n",
      "Epoch 81 Batch 100 Loss 0.2072 Accuracy 0.9387\n",
      "Epoch 81 Batch 150 Loss 0.1988 Accuracy 0.9409\n",
      "Epoch 81 Batch 200 Loss 0.1923 Accuracy 0.9427\n",
      "Epoch 81 Batch 250 Loss 0.1890 Accuracy 0.9439\n",
      "Epoch 81 Batch 300 Loss 0.1881 Accuracy 0.9444\n",
      "Epoch 81 Batch 350 Loss 0.1883 Accuracy 0.9444\n",
      "Epoch 81 Batch 400 Loss 0.1881 Accuracy 0.9447\n",
      "Epoch 81 Batch 450 Loss 0.1899 Accuracy 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 500 Loss 0.1912 Accuracy 0.9443\n",
      "Epoch 81 Loss 0.1922 Accuracy 0.9442\n",
      "Time taken for 1 epoch: 158.78654837608337 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.1687 Accuracy 0.9381\n",
      "Epoch 82 Batch 50 Loss 0.2058 Accuracy 0.9388\n",
      "Epoch 82 Batch 100 Loss 0.2047 Accuracy 0.9390\n",
      "Epoch 82 Batch 150 Loss 0.1957 Accuracy 0.9410\n",
      "Epoch 82 Batch 200 Loss 0.1901 Accuracy 0.9429\n",
      "Epoch 82 Batch 250 Loss 0.1865 Accuracy 0.9448\n",
      "Epoch 82 Batch 300 Loss 0.1845 Accuracy 0.9455\n",
      "Epoch 82 Batch 350 Loss 0.1849 Accuracy 0.9455\n",
      "Epoch 82 Batch 400 Loss 0.1860 Accuracy 0.9456\n",
      "Epoch 82 Batch 450 Loss 0.1863 Accuracy 0.9456\n",
      "Epoch 82 Batch 500 Loss 0.1885 Accuracy 0.9451\n",
      "Epoch 82 Loss 0.1888 Accuracy 0.9450\n",
      "Time taken for 1 epoch: 158.35846996307373 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.2141 Accuracy 0.9293\n",
      "Epoch 83 Batch 50 Loss 0.1930 Accuracy 0.9426\n",
      "Epoch 83 Batch 100 Loss 0.1914 Accuracy 0.9416\n",
      "Epoch 83 Batch 150 Loss 0.1848 Accuracy 0.9436\n",
      "Epoch 83 Batch 200 Loss 0.1817 Accuracy 0.9452\n",
      "Epoch 83 Batch 250 Loss 0.1789 Accuracy 0.9461\n",
      "Epoch 83 Batch 300 Loss 0.1810 Accuracy 0.9462\n",
      "Epoch 83 Batch 350 Loss 0.1797 Accuracy 0.9469\n",
      "Epoch 83 Batch 400 Loss 0.1820 Accuracy 0.9465\n",
      "Epoch 83 Batch 450 Loss 0.1831 Accuracy 0.9465\n",
      "Epoch 83 Batch 500 Loss 0.1854 Accuracy 0.9461\n",
      "Epoch 83 Loss 0.1864 Accuracy 0.9460\n",
      "Time taken for 1 epoch: 156.55464553833008 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.2046 Accuracy 0.9213\n",
      "Epoch 84 Batch 50 Loss 0.1958 Accuracy 0.9403\n",
      "Epoch 84 Batch 100 Loss 0.1999 Accuracy 0.9396\n",
      "Epoch 84 Batch 150 Loss 0.1930 Accuracy 0.9419\n",
      "Epoch 84 Batch 200 Loss 0.1875 Accuracy 0.9442\n",
      "Epoch 84 Batch 250 Loss 0.1836 Accuracy 0.9454\n",
      "Epoch 84 Batch 300 Loss 0.1834 Accuracy 0.9457\n",
      "Epoch 84 Batch 350 Loss 0.1820 Accuracy 0.9463\n",
      "Epoch 84 Batch 400 Loss 0.1825 Accuracy 0.9463\n",
      "Epoch 84 Batch 450 Loss 0.1829 Accuracy 0.9463\n",
      "Epoch 84 Batch 500 Loss 0.1835 Accuracy 0.9461\n",
      "Epoch 84 Loss 0.1838 Accuracy 0.9459\n",
      "Time taken for 1 epoch: 159.05172491073608 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.2162 Accuracy 0.9371\n",
      "Epoch 85 Batch 50 Loss 0.1951 Accuracy 0.9411\n",
      "Epoch 85 Batch 100 Loss 0.1949 Accuracy 0.9416\n",
      "Epoch 85 Batch 150 Loss 0.1906 Accuracy 0.9431\n",
      "Epoch 85 Batch 200 Loss 0.1839 Accuracy 0.9450\n",
      "Epoch 85 Batch 250 Loss 0.1826 Accuracy 0.9456\n",
      "Epoch 85 Batch 300 Loss 0.1801 Accuracy 0.9469\n",
      "Epoch 85 Batch 350 Loss 0.1793 Accuracy 0.9474\n",
      "Epoch 85 Batch 400 Loss 0.1783 Accuracy 0.9479\n",
      "Epoch 85 Batch 450 Loss 0.1791 Accuracy 0.9480\n",
      "Epoch 85 Batch 500 Loss 0.1808 Accuracy 0.9476\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train\\ckpt-17\n",
      "Epoch 85 Loss 0.1818 Accuracy 0.9474\n",
      "Time taken for 1 epoch: 169.77076768875122 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.1209 Accuracy 0.9681\n",
      "Epoch 86 Batch 50 Loss 0.1904 Accuracy 0.9413\n",
      "Epoch 86 Batch 100 Loss 0.1844 Accuracy 0.9451\n",
      "Epoch 86 Batch 150 Loss 0.1860 Accuracy 0.9448\n",
      "Epoch 86 Batch 200 Loss 0.1812 Accuracy 0.9464\n",
      "Epoch 86 Batch 250 Loss 0.1758 Accuracy 0.9484\n",
      "Epoch 86 Batch 300 Loss 0.1741 Accuracy 0.9491\n",
      "Epoch 86 Batch 350 Loss 0.1744 Accuracy 0.9494\n",
      "Epoch 86 Batch 400 Loss 0.1746 Accuracy 0.9494\n",
      "Epoch 86 Batch 450 Loss 0.1755 Accuracy 0.9492\n",
      "Epoch 86 Batch 500 Loss 0.1759 Accuracy 0.9491\n",
      "Epoch 86 Loss 0.1770 Accuracy 0.9489\n",
      "Time taken for 1 epoch: 161.222576379776 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.1808 Accuracy 0.9606\n",
      "Epoch 87 Batch 50 Loss 0.1885 Accuracy 0.9435\n",
      "Epoch 87 Batch 100 Loss 0.1875 Accuracy 0.9450\n",
      "Epoch 87 Batch 150 Loss 0.1870 Accuracy 0.9450\n",
      "Epoch 87 Batch 200 Loss 0.1806 Accuracy 0.9473\n",
      "Epoch 87 Batch 250 Loss 0.1741 Accuracy 0.9493\n",
      "Epoch 87 Batch 300 Loss 0.1726 Accuracy 0.9498\n",
      "Epoch 87 Batch 350 Loss 0.1722 Accuracy 0.9500\n",
      "Epoch 87 Batch 400 Loss 0.1713 Accuracy 0.9502\n",
      "Epoch 87 Batch 450 Loss 0.1735 Accuracy 0.9497\n",
      "Epoch 87 Batch 500 Loss 0.1755 Accuracy 0.9490\n",
      "Epoch 87 Loss 0.1764 Accuracy 0.9489\n",
      "Time taken for 1 epoch: 157.21721172332764 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.1602 Accuracy 0.9522\n",
      "Epoch 88 Batch 50 Loss 0.1872 Accuracy 0.9441\n",
      "Epoch 88 Batch 100 Loss 0.1821 Accuracy 0.9459\n",
      "Epoch 88 Batch 150 Loss 0.1800 Accuracy 0.9468\n",
      "Epoch 88 Batch 200 Loss 0.1763 Accuracy 0.9480\n",
      "Epoch 88 Batch 250 Loss 0.1709 Accuracy 0.9500\n",
      "Epoch 88 Batch 300 Loss 0.1693 Accuracy 0.9508\n",
      "Epoch 88 Batch 350 Loss 0.1695 Accuracy 0.9509\n",
      "Epoch 88 Batch 400 Loss 0.1701 Accuracy 0.9508\n",
      "Epoch 88 Batch 450 Loss 0.1712 Accuracy 0.9507\n",
      "Epoch 88 Batch 500 Loss 0.1716 Accuracy 0.9505\n",
      "Epoch 88 Loss 0.1723 Accuracy 0.9503\n",
      "Time taken for 1 epoch: 158.08885264396667 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.2455 Accuracy 0.9201\n",
      "Epoch 89 Batch 50 Loss 0.1881 Accuracy 0.9412\n",
      "Epoch 89 Batch 100 Loss 0.1815 Accuracy 0.9448\n",
      "Epoch 89 Batch 150 Loss 0.1788 Accuracy 0.9462\n",
      "Epoch 89 Batch 200 Loss 0.1738 Accuracy 0.9485\n",
      "Epoch 89 Batch 250 Loss 0.1711 Accuracy 0.9492\n",
      "Epoch 89 Batch 300 Loss 0.1705 Accuracy 0.9497\n",
      "Epoch 89 Batch 350 Loss 0.1716 Accuracy 0.9498\n",
      "Epoch 89 Batch 400 Loss 0.1719 Accuracy 0.9500\n",
      "Epoch 89 Batch 450 Loss 0.1719 Accuracy 0.9502\n",
      "Epoch 89 Batch 500 Loss 0.1729 Accuracy 0.9500\n",
      "Epoch 89 Loss 0.1735 Accuracy 0.9498\n",
      "Time taken for 1 epoch: 156.6128351688385 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.1254 Accuracy 0.9612\n",
      "Epoch 90 Batch 50 Loss 0.1864 Accuracy 0.9463\n",
      "Epoch 90 Batch 100 Loss 0.1819 Accuracy 0.9464\n",
      "Epoch 90 Batch 150 Loss 0.1775 Accuracy 0.9480\n",
      "Epoch 90 Batch 200 Loss 0.1734 Accuracy 0.9490\n",
      "Epoch 90 Batch 250 Loss 0.1693 Accuracy 0.9504\n",
      "Epoch 90 Batch 300 Loss 0.1686 Accuracy 0.9507\n",
      "Epoch 90 Batch 350 Loss 0.1682 Accuracy 0.9511\n",
      "Epoch 90 Batch 400 Loss 0.1678 Accuracy 0.9514\n",
      "Epoch 90 Batch 450 Loss 0.1697 Accuracy 0.9508\n",
      "Epoch 90 Batch 500 Loss 0.1707 Accuracy 0.9507\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train\\ckpt-18\n",
      "Epoch 90 Loss 0.1703 Accuracy 0.9508\n",
      "Time taken for 1 epoch: 161.58102440834045 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.2106 Accuracy 0.9317\n",
      "Epoch 91 Batch 50 Loss 0.1775 Accuracy 0.9471\n",
      "Epoch 91 Batch 100 Loss 0.1795 Accuracy 0.9467\n",
      "Epoch 91 Batch 150 Loss 0.1763 Accuracy 0.9484\n",
      "Epoch 91 Batch 200 Loss 0.1715 Accuracy 0.9502\n",
      "Epoch 91 Batch 250 Loss 0.1679 Accuracy 0.9509\n",
      "Epoch 91 Batch 300 Loss 0.1655 Accuracy 0.9516\n",
      "Epoch 91 Batch 350 Loss 0.1648 Accuracy 0.9518\n",
      "Epoch 91 Batch 400 Loss 0.1644 Accuracy 0.9520\n",
      "Epoch 91 Batch 450 Loss 0.1648 Accuracy 0.9521\n",
      "Epoch 91 Batch 500 Loss 0.1660 Accuracy 0.9519\n",
      "Epoch 91 Loss 0.1665 Accuracy 0.9519\n",
      "Time taken for 1 epoch: 152.5702030658722 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.1368 Accuracy 0.9558\n",
      "Epoch 92 Batch 50 Loss 0.1778 Accuracy 0.9468\n",
      "Epoch 92 Batch 100 Loss 0.1804 Accuracy 0.9458\n",
      "Epoch 92 Batch 150 Loss 0.1744 Accuracy 0.9482\n",
      "Epoch 92 Batch 200 Loss 0.1679 Accuracy 0.9505\n",
      "Epoch 92 Batch 250 Loss 0.1655 Accuracy 0.9511\n",
      "Epoch 92 Batch 300 Loss 0.1636 Accuracy 0.9520\n",
      "Epoch 92 Batch 350 Loss 0.1646 Accuracy 0.9522\n",
      "Epoch 92 Batch 400 Loss 0.1661 Accuracy 0.9519\n",
      "Epoch 92 Batch 450 Loss 0.1663 Accuracy 0.9521\n",
      "Epoch 92 Batch 500 Loss 0.1676 Accuracy 0.9519\n",
      "Epoch 92 Loss 0.1676 Accuracy 0.9520\n",
      "Time taken for 1 epoch: 154.13699674606323 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.1815 Accuracy 0.9362\n",
      "Epoch 93 Batch 50 Loss 0.1796 Accuracy 0.9464\n",
      "Epoch 93 Batch 100 Loss 0.1752 Accuracy 0.9479\n",
      "Epoch 93 Batch 150 Loss 0.1714 Accuracy 0.9494\n",
      "Epoch 93 Batch 200 Loss 0.1670 Accuracy 0.9510\n",
      "Epoch 93 Batch 250 Loss 0.1657 Accuracy 0.9518\n",
      "Epoch 93 Batch 300 Loss 0.1630 Accuracy 0.9525\n",
      "Epoch 93 Batch 350 Loss 0.1603 Accuracy 0.9534\n",
      "Epoch 93 Batch 400 Loss 0.1611 Accuracy 0.9533\n",
      "Epoch 93 Batch 450 Loss 0.1623 Accuracy 0.9531\n",
      "Epoch 93 Batch 500 Loss 0.1630 Accuracy 0.9532\n",
      "Epoch 93 Loss 0.1635 Accuracy 0.9531\n",
      "Time taken for 1 epoch: 153.59127259254456 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.1143 Accuracy 0.9675\n",
      "Epoch 94 Batch 50 Loss 0.1686 Accuracy 0.9493\n",
      "Epoch 94 Batch 100 Loss 0.1683 Accuracy 0.9493\n",
      "Epoch 94 Batch 150 Loss 0.1677 Accuracy 0.9501\n",
      "Epoch 94 Batch 200 Loss 0.1650 Accuracy 0.9515\n",
      "Epoch 94 Batch 250 Loss 0.1612 Accuracy 0.9527\n",
      "Epoch 94 Batch 300 Loss 0.1596 Accuracy 0.9535\n",
      "Epoch 94 Batch 350 Loss 0.1603 Accuracy 0.9535\n",
      "Epoch 94 Batch 400 Loss 0.1614 Accuracy 0.9533\n",
      "Epoch 94 Batch 450 Loss 0.1623 Accuracy 0.9533\n",
      "Epoch 94 Batch 500 Loss 0.1638 Accuracy 0.9529\n",
      "Epoch 94 Loss 0.1645 Accuracy 0.9528\n",
      "Time taken for 1 epoch: 154.04387950897217 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.2584 Accuracy 0.9141\n",
      "Epoch 95 Batch 50 Loss 0.1675 Accuracy 0.9486\n",
      "Epoch 95 Batch 100 Loss 0.1643 Accuracy 0.9504\n",
      "Epoch 95 Batch 150 Loss 0.1638 Accuracy 0.9508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 200 Loss 0.1620 Accuracy 0.9517\n",
      "Epoch 95 Batch 250 Loss 0.1602 Accuracy 0.9528\n",
      "Epoch 95 Batch 300 Loss 0.1579 Accuracy 0.9535\n",
      "Epoch 95 Batch 350 Loss 0.1567 Accuracy 0.9541\n",
      "Epoch 95 Batch 400 Loss 0.1572 Accuracy 0.9542\n",
      "Epoch 95 Batch 450 Loss 0.1579 Accuracy 0.9543\n",
      "Epoch 95 Batch 500 Loss 0.1592 Accuracy 0.9540\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train\\ckpt-19\n",
      "Epoch 95 Loss 0.1604 Accuracy 0.9538\n",
      "Time taken for 1 epoch: 161.08275389671326 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.2272 Accuracy 0.9414\n",
      "Epoch 96 Batch 50 Loss 0.1695 Accuracy 0.9493\n",
      "Epoch 96 Batch 100 Loss 0.1640 Accuracy 0.9515\n",
      "Epoch 96 Batch 150 Loss 0.1616 Accuracy 0.9517\n",
      "Epoch 96 Batch 200 Loss 0.1595 Accuracy 0.9529\n",
      "Epoch 96 Batch 250 Loss 0.1583 Accuracy 0.9535\n",
      "Epoch 96 Batch 300 Loss 0.1564 Accuracy 0.9542\n",
      "Epoch 96 Batch 350 Loss 0.1571 Accuracy 0.9542\n",
      "Epoch 96 Batch 400 Loss 0.1577 Accuracy 0.9543\n",
      "Epoch 96 Batch 450 Loss 0.1580 Accuracy 0.9544\n",
      "Epoch 96 Batch 500 Loss 0.1587 Accuracy 0.9545\n",
      "Epoch 96 Loss 0.1599 Accuracy 0.9542\n",
      "Time taken for 1 epoch: 154.10271430015564 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.1965 Accuracy 0.9254\n",
      "Epoch 97 Batch 50 Loss 0.1775 Accuracy 0.9465\n",
      "Epoch 97 Batch 100 Loss 0.1667 Accuracy 0.9508\n",
      "Epoch 97 Batch 150 Loss 0.1644 Accuracy 0.9522\n",
      "Epoch 97 Batch 200 Loss 0.1579 Accuracy 0.9543\n",
      "Epoch 97 Batch 250 Loss 0.1528 Accuracy 0.9557\n",
      "Epoch 97 Batch 300 Loss 0.1525 Accuracy 0.9559\n",
      "Epoch 97 Batch 350 Loss 0.1530 Accuracy 0.9559\n",
      "Epoch 97 Batch 400 Loss 0.1534 Accuracy 0.9560\n",
      "Epoch 97 Batch 450 Loss 0.1559 Accuracy 0.9554\n",
      "Epoch 97 Batch 500 Loss 0.1576 Accuracy 0.9549\n",
      "Epoch 97 Loss 0.1585 Accuracy 0.9548\n",
      "Time taken for 1 epoch: 153.90337300300598 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.2440 Accuracy 0.9386\n",
      "Epoch 98 Batch 50 Loss 0.1698 Accuracy 0.9478\n",
      "Epoch 98 Batch 100 Loss 0.1666 Accuracy 0.9501\n",
      "Epoch 98 Batch 150 Loss 0.1643 Accuracy 0.9512\n",
      "Epoch 98 Batch 200 Loss 0.1585 Accuracy 0.9535\n",
      "Epoch 98 Batch 250 Loss 0.1568 Accuracy 0.9542\n",
      "Epoch 98 Batch 300 Loss 0.1542 Accuracy 0.9552\n",
      "Epoch 98 Batch 350 Loss 0.1546 Accuracy 0.9553\n",
      "Epoch 98 Batch 400 Loss 0.1558 Accuracy 0.9551\n",
      "Epoch 98 Batch 450 Loss 0.1568 Accuracy 0.9550\n",
      "Epoch 98 Batch 500 Loss 0.1573 Accuracy 0.9548\n",
      "Epoch 98 Loss 0.1583 Accuracy 0.9546\n",
      "Time taken for 1 epoch: 153.82305002212524 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.1496 Accuracy 0.9571\n",
      "Epoch 99 Batch 50 Loss 0.1586 Accuracy 0.9513\n",
      "Epoch 99 Batch 100 Loss 0.1658 Accuracy 0.9500\n",
      "Epoch 99 Batch 150 Loss 0.1612 Accuracy 0.9516\n",
      "Epoch 99 Batch 200 Loss 0.1567 Accuracy 0.9533\n",
      "Epoch 99 Batch 250 Loss 0.1549 Accuracy 0.9542\n",
      "Epoch 99 Batch 300 Loss 0.1536 Accuracy 0.9547\n",
      "Epoch 99 Batch 350 Loss 0.1528 Accuracy 0.9552\n",
      "Epoch 99 Batch 400 Loss 0.1543 Accuracy 0.9550\n",
      "Epoch 99 Batch 450 Loss 0.1547 Accuracy 0.9550\n",
      "Epoch 99 Batch 500 Loss 0.1573 Accuracy 0.9546\n",
      "Epoch 99 Loss 0.1577 Accuracy 0.9545\n",
      "Time taken for 1 epoch: 154.26252627372742 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.1313 Accuracy 0.9609\n",
      "Epoch 100 Batch 50 Loss 0.1628 Accuracy 0.9514\n",
      "Epoch 100 Batch 100 Loss 0.1611 Accuracy 0.9518\n",
      "Epoch 100 Batch 150 Loss 0.1562 Accuracy 0.9537\n",
      "Epoch 100 Batch 200 Loss 0.1506 Accuracy 0.9558\n",
      "Epoch 100 Batch 250 Loss 0.1489 Accuracy 0.9564\n",
      "Epoch 100 Batch 300 Loss 0.1500 Accuracy 0.9563\n",
      "Epoch 100 Batch 350 Loss 0.1501 Accuracy 0.9562\n",
      "Epoch 100 Batch 400 Loss 0.1509 Accuracy 0.9559\n",
      "Epoch 100 Batch 450 Loss 0.1523 Accuracy 0.9558\n",
      "Epoch 100 Batch 500 Loss 0.1536 Accuracy 0.9556\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train\\ckpt-20\n",
      "Epoch 100 Loss 0.1540 Accuracy 0.9556\n",
      "Time taken for 1 epoch: 160.81958174705505 secs\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.1443 Accuracy 0.9590\n",
      "Epoch 101 Batch 50 Loss 0.1690 Accuracy 0.9491\n",
      "Epoch 101 Batch 100 Loss 0.1648 Accuracy 0.9509\n",
      "Epoch 101 Batch 150 Loss 0.1614 Accuracy 0.9524\n",
      "Epoch 101 Batch 200 Loss 0.1566 Accuracy 0.9540\n",
      "Epoch 101 Batch 250 Loss 0.1518 Accuracy 0.9554\n",
      "Epoch 101 Batch 300 Loss 0.1497 Accuracy 0.9562\n",
      "Epoch 101 Batch 350 Loss 0.1515 Accuracy 0.9560\n",
      "Epoch 101 Batch 400 Loss 0.1530 Accuracy 0.9558\n",
      "Epoch 101 Batch 450 Loss 0.1529 Accuracy 0.9559\n",
      "Epoch 101 Batch 500 Loss 0.1540 Accuracy 0.9557\n",
      "Epoch 101 Loss 0.1538 Accuracy 0.9559\n",
      "Time taken for 1 epoch: 153.46286511421204 secs\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.1144 Accuracy 0.9669\n",
      "Epoch 102 Batch 50 Loss 0.1574 Accuracy 0.9539\n",
      "Epoch 102 Batch 100 Loss 0.1596 Accuracy 0.9536\n",
      "Epoch 102 Batch 150 Loss 0.1559 Accuracy 0.9552\n",
      "Epoch 102 Batch 200 Loss 0.1561 Accuracy 0.9551\n",
      "Epoch 102 Batch 250 Loss 0.1525 Accuracy 0.9563\n",
      "Epoch 102 Batch 300 Loss 0.1516 Accuracy 0.9570\n",
      "Epoch 102 Batch 350 Loss 0.1514 Accuracy 0.9569\n",
      "Epoch 102 Batch 400 Loss 0.1508 Accuracy 0.9572\n",
      "Epoch 102 Batch 450 Loss 0.1509 Accuracy 0.9572\n",
      "Epoch 102 Batch 500 Loss 0.1521 Accuracy 0.9570\n",
      "Epoch 102 Loss 0.1528 Accuracy 0.9568\n",
      "Time taken for 1 epoch: 153.54294228553772 secs\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.1346 Accuracy 0.9656\n",
      "Epoch 103 Batch 50 Loss 0.1528 Accuracy 0.9540\n",
      "Epoch 103 Batch 100 Loss 0.1501 Accuracy 0.9546\n",
      "Epoch 103 Batch 150 Loss 0.1507 Accuracy 0.9553\n",
      "Epoch 103 Batch 200 Loss 0.1462 Accuracy 0.9572\n",
      "Epoch 103 Batch 250 Loss 0.1459 Accuracy 0.9575\n",
      "Epoch 103 Batch 300 Loss 0.1459 Accuracy 0.9576\n",
      "Epoch 103 Batch 350 Loss 0.1464 Accuracy 0.9576\n",
      "Epoch 103 Batch 400 Loss 0.1477 Accuracy 0.9572\n",
      "Epoch 103 Batch 450 Loss 0.1486 Accuracy 0.9570\n",
      "Epoch 103 Batch 500 Loss 0.1496 Accuracy 0.9569\n",
      "Epoch 103 Loss 0.1507 Accuracy 0.9566\n",
      "Time taken for 1 epoch: 154.27769207954407 secs\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.1405 Accuracy 0.9573\n",
      "Epoch 104 Batch 50 Loss 0.1494 Accuracy 0.9558\n",
      "Epoch 104 Batch 100 Loss 0.1455 Accuracy 0.9565\n",
      "Epoch 104 Batch 150 Loss 0.1456 Accuracy 0.9565\n",
      "Epoch 104 Batch 200 Loss 0.1440 Accuracy 0.9571\n",
      "Epoch 104 Batch 250 Loss 0.1422 Accuracy 0.9583\n",
      "Epoch 104 Batch 300 Loss 0.1417 Accuracy 0.9585\n",
      "Epoch 104 Batch 350 Loss 0.1417 Accuracy 0.9587\n",
      "Epoch 104 Batch 400 Loss 0.1424 Accuracy 0.9588\n",
      "Epoch 104 Batch 450 Loss 0.1424 Accuracy 0.9589\n",
      "Epoch 104 Batch 500 Loss 0.1441 Accuracy 0.9587\n",
      "Epoch 104 Loss 0.1453 Accuracy 0.9584\n",
      "Time taken for 1 epoch: 154.07651257514954 secs\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.1856 Accuracy 0.9200\n",
      "Epoch 105 Batch 50 Loss 0.1754 Accuracy 0.9479\n",
      "Epoch 105 Batch 100 Loss 0.1687 Accuracy 0.9495\n",
      "Epoch 105 Batch 150 Loss 0.1583 Accuracy 0.9531\n",
      "Epoch 105 Batch 200 Loss 0.1538 Accuracy 0.9545\n",
      "Epoch 105 Batch 250 Loss 0.1497 Accuracy 0.9558\n",
      "Epoch 105 Batch 300 Loss 0.1476 Accuracy 0.9566\n",
      "Epoch 105 Batch 350 Loss 0.1481 Accuracy 0.9566\n",
      "Epoch 105 Batch 400 Loss 0.1479 Accuracy 0.9568\n",
      "Epoch 105 Batch 450 Loss 0.1482 Accuracy 0.9571\n",
      "Epoch 105 Batch 500 Loss 0.1482 Accuracy 0.9570\n",
      "Saving checkpoint for epoch 105 at ./checkpoints/train\\ckpt-21\n",
      "Epoch 105 Loss 0.1484 Accuracy 0.9570\n",
      "Time taken for 1 epoch: 160.67509937286377 secs\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.1144 Accuracy 0.9660\n",
      "Epoch 106 Batch 50 Loss 0.1491 Accuracy 0.9556\n",
      "Epoch 106 Batch 100 Loss 0.1498 Accuracy 0.9552\n",
      "Epoch 106 Batch 150 Loss 0.1479 Accuracy 0.9567\n",
      "Epoch 106 Batch 200 Loss 0.1457 Accuracy 0.9573\n",
      "Epoch 106 Batch 250 Loss 0.1442 Accuracy 0.9580\n",
      "Epoch 106 Batch 300 Loss 0.1429 Accuracy 0.9586\n",
      "Epoch 106 Batch 350 Loss 0.1428 Accuracy 0.9588\n",
      "Epoch 106 Batch 400 Loss 0.1436 Accuracy 0.9587\n",
      "Epoch 106 Batch 450 Loss 0.1446 Accuracy 0.9587\n",
      "Epoch 106 Batch 500 Loss 0.1468 Accuracy 0.9582\n",
      "Epoch 106 Loss 0.1471 Accuracy 0.9582\n",
      "Time taken for 1 epoch: 153.80347418785095 secs\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.1020 Accuracy 0.9674\n",
      "Epoch 107 Batch 50 Loss 0.1458 Accuracy 0.9575\n",
      "Epoch 107 Batch 100 Loss 0.1496 Accuracy 0.9565\n",
      "Epoch 107 Batch 150 Loss 0.1463 Accuracy 0.9578\n",
      "Epoch 107 Batch 200 Loss 0.1419 Accuracy 0.9589\n",
      "Epoch 107 Batch 250 Loss 0.1393 Accuracy 0.9600\n",
      "Epoch 107 Batch 300 Loss 0.1409 Accuracy 0.9599\n",
      "Epoch 107 Batch 350 Loss 0.1408 Accuracy 0.9601\n",
      "Epoch 107 Batch 400 Loss 0.1414 Accuracy 0.9600\n",
      "Epoch 107 Batch 450 Loss 0.1425 Accuracy 0.9595\n",
      "Epoch 107 Batch 500 Loss 0.1438 Accuracy 0.9593\n",
      "Epoch 107 Loss 0.1435 Accuracy 0.9594\n",
      "Time taken for 1 epoch: 153.51475286483765 secs\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.1630 Accuracy 0.9579\n",
      "Epoch 108 Batch 50 Loss 0.1493 Accuracy 0.9549\n",
      "Epoch 108 Batch 100 Loss 0.1500 Accuracy 0.9554\n",
      "Epoch 108 Batch 150 Loss 0.1474 Accuracy 0.9562\n",
      "Epoch 108 Batch 200 Loss 0.1453 Accuracy 0.9570\n",
      "Epoch 108 Batch 250 Loss 0.1411 Accuracy 0.9584\n",
      "Epoch 108 Batch 300 Loss 0.1403 Accuracy 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 Batch 350 Loss 0.1398 Accuracy 0.9595\n",
      "Epoch 108 Batch 400 Loss 0.1403 Accuracy 0.9594\n",
      "Epoch 108 Batch 450 Loss 0.1415 Accuracy 0.9593\n",
      "Epoch 108 Batch 500 Loss 0.1427 Accuracy 0.9591\n",
      "Epoch 108 Loss 0.1435 Accuracy 0.9590\n",
      "Time taken for 1 epoch: 153.86160397529602 secs\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.1703 Accuracy 0.9626\n",
      "Epoch 109 Batch 50 Loss 0.1599 Accuracy 0.9531\n",
      "Epoch 109 Batch 100 Loss 0.1538 Accuracy 0.9544\n",
      "Epoch 109 Batch 150 Loss 0.1501 Accuracy 0.9556\n",
      "Epoch 109 Batch 200 Loss 0.1452 Accuracy 0.9575\n",
      "Epoch 109 Batch 250 Loss 0.1417 Accuracy 0.9587\n",
      "Epoch 109 Batch 300 Loss 0.1410 Accuracy 0.9589\n",
      "Epoch 109 Batch 350 Loss 0.1410 Accuracy 0.9591\n",
      "Epoch 109 Batch 400 Loss 0.1419 Accuracy 0.9591\n",
      "Epoch 109 Batch 450 Loss 0.1420 Accuracy 0.9594\n",
      "Epoch 109 Batch 500 Loss 0.1434 Accuracy 0.9592\n",
      "Epoch 109 Loss 0.1439 Accuracy 0.9592\n",
      "Time taken for 1 epoch: 153.01341676712036 secs\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.1259 Accuracy 0.9652\n",
      "Epoch 110 Batch 50 Loss 0.1437 Accuracy 0.9565\n",
      "Epoch 110 Batch 100 Loss 0.1468 Accuracy 0.9566\n",
      "Epoch 110 Batch 150 Loss 0.1473 Accuracy 0.9575\n",
      "Epoch 110 Batch 200 Loss 0.1416 Accuracy 0.9594\n",
      "Epoch 110 Batch 250 Loss 0.1383 Accuracy 0.9606\n",
      "Epoch 110 Batch 300 Loss 0.1372 Accuracy 0.9612\n",
      "Epoch 110 Batch 350 Loss 0.1370 Accuracy 0.9613\n",
      "Epoch 110 Batch 400 Loss 0.1362 Accuracy 0.9615\n",
      "Epoch 110 Batch 450 Loss 0.1379 Accuracy 0.9612\n",
      "Epoch 110 Batch 500 Loss 0.1378 Accuracy 0.9614\n",
      "Saving checkpoint for epoch 110 at ./checkpoints/train\\ckpt-22\n",
      "Epoch 110 Loss 0.1381 Accuracy 0.9614\n",
      "Time taken for 1 epoch: 160.37980008125305 secs\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.1479 Accuracy 0.9547\n",
      "Epoch 111 Batch 50 Loss 0.1503 Accuracy 0.9568\n",
      "Epoch 111 Batch 100 Loss 0.1530 Accuracy 0.9554\n",
      "Epoch 111 Batch 150 Loss 0.1458 Accuracy 0.9575\n",
      "Epoch 111 Batch 200 Loss 0.1416 Accuracy 0.9588\n",
      "Epoch 111 Batch 250 Loss 0.1395 Accuracy 0.9598\n",
      "Epoch 111 Batch 300 Loss 0.1385 Accuracy 0.9603\n",
      "Epoch 111 Batch 350 Loss 0.1394 Accuracy 0.9602\n",
      "Epoch 111 Batch 400 Loss 0.1391 Accuracy 0.9604\n",
      "Epoch 111 Batch 450 Loss 0.1397 Accuracy 0.9602\n",
      "Epoch 111 Batch 500 Loss 0.1400 Accuracy 0.9601\n",
      "Epoch 111 Loss 0.1405 Accuracy 0.9600\n",
      "Time taken for 1 epoch: 159.3369755744934 secs\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.1290 Accuracy 0.9793\n",
      "Epoch 112 Batch 50 Loss 0.1477 Accuracy 0.9560\n",
      "Epoch 112 Batch 100 Loss 0.1447 Accuracy 0.9577\n",
      "Epoch 112 Batch 150 Loss 0.1393 Accuracy 0.9590\n",
      "Epoch 112 Batch 200 Loss 0.1374 Accuracy 0.9602\n",
      "Epoch 112 Batch 250 Loss 0.1351 Accuracy 0.9607\n",
      "Epoch 112 Batch 300 Loss 0.1364 Accuracy 0.9606\n",
      "Epoch 112 Batch 350 Loss 0.1361 Accuracy 0.9608\n",
      "Epoch 112 Batch 400 Loss 0.1360 Accuracy 0.9610\n",
      "Epoch 112 Batch 450 Loss 0.1360 Accuracy 0.9611\n",
      "Epoch 112 Batch 500 Loss 0.1370 Accuracy 0.9609\n",
      "Epoch 112 Loss 0.1372 Accuracy 0.9609\n",
      "Time taken for 1 epoch: 158.3880546092987 secs\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0987 Accuracy 0.9798\n",
      "Epoch 113 Batch 50 Loss 0.1516 Accuracy 0.9555\n",
      "Epoch 113 Batch 100 Loss 0.1477 Accuracy 0.9562\n",
      "Epoch 113 Batch 150 Loss 0.1472 Accuracy 0.9567\n",
      "Epoch 113 Batch 200 Loss 0.1428 Accuracy 0.9580\n",
      "Epoch 113 Batch 250 Loss 0.1361 Accuracy 0.9600\n",
      "Epoch 113 Batch 300 Loss 0.1359 Accuracy 0.9605\n",
      "Epoch 113 Batch 350 Loss 0.1358 Accuracy 0.9606\n",
      "Epoch 113 Batch 400 Loss 0.1358 Accuracy 0.9608\n",
      "Epoch 113 Batch 450 Loss 0.1357 Accuracy 0.9609\n",
      "Epoch 113 Batch 500 Loss 0.1363 Accuracy 0.9609\n",
      "Epoch 113 Loss 0.1367 Accuracy 0.9607\n",
      "Time taken for 1 epoch: 158.5316677093506 secs\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.1481 Accuracy 0.9528\n",
      "Epoch 114 Batch 50 Loss 0.1422 Accuracy 0.9577\n",
      "Epoch 114 Batch 100 Loss 0.1457 Accuracy 0.9566\n",
      "Epoch 114 Batch 150 Loss 0.1403 Accuracy 0.9587\n",
      "Epoch 114 Batch 200 Loss 0.1339 Accuracy 0.9608\n",
      "Epoch 114 Batch 250 Loss 0.1325 Accuracy 0.9615\n",
      "Epoch 114 Batch 300 Loss 0.1330 Accuracy 0.9617\n",
      "Epoch 114 Batch 350 Loss 0.1334 Accuracy 0.9617\n",
      "Epoch 114 Batch 400 Loss 0.1353 Accuracy 0.9615\n",
      "Epoch 114 Batch 450 Loss 0.1350 Accuracy 0.9616\n",
      "Epoch 114 Batch 500 Loss 0.1355 Accuracy 0.9615\n",
      "Epoch 114 Loss 0.1361 Accuracy 0.9615\n",
      "Time taken for 1 epoch: 158.99165153503418 secs\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.1060 Accuracy 0.9764\n",
      "Epoch 115 Batch 50 Loss 0.1442 Accuracy 0.9571\n",
      "Epoch 115 Batch 100 Loss 0.1451 Accuracy 0.9578\n",
      "Epoch 115 Batch 150 Loss 0.1397 Accuracy 0.9595\n",
      "Epoch 115 Batch 200 Loss 0.1370 Accuracy 0.9603\n",
      "Epoch 115 Batch 250 Loss 0.1347 Accuracy 0.9614\n",
      "Epoch 115 Batch 300 Loss 0.1345 Accuracy 0.9620\n",
      "Epoch 115 Batch 350 Loss 0.1341 Accuracy 0.9623\n",
      "Epoch 115 Batch 400 Loss 0.1331 Accuracy 0.9625\n",
      "Epoch 115 Batch 450 Loss 0.1335 Accuracy 0.9625\n",
      "Epoch 115 Batch 500 Loss 0.1346 Accuracy 0.9622\n",
      "Saving checkpoint for epoch 115 at ./checkpoints/train\\ckpt-23\n",
      "Epoch 115 Loss 0.1348 Accuracy 0.9621\n",
      "Time taken for 1 epoch: 165.19620084762573 secs\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.1673 Accuracy 0.9439\n",
      "Epoch 116 Batch 50 Loss 0.1459 Accuracy 0.9568\n",
      "Epoch 116 Batch 100 Loss 0.1400 Accuracy 0.9593\n",
      "Epoch 116 Batch 150 Loss 0.1365 Accuracy 0.9605\n",
      "Epoch 116 Batch 200 Loss 0.1331 Accuracy 0.9618\n",
      "Epoch 116 Batch 250 Loss 0.1306 Accuracy 0.9627\n",
      "Epoch 116 Batch 300 Loss 0.1304 Accuracy 0.9627\n",
      "Epoch 116 Batch 350 Loss 0.1315 Accuracy 0.9623\n",
      "Epoch 116 Batch 400 Loss 0.1327 Accuracy 0.9620\n",
      "Epoch 116 Batch 450 Loss 0.1332 Accuracy 0.9620\n",
      "Epoch 116 Batch 500 Loss 0.1335 Accuracy 0.9620\n",
      "Epoch 116 Loss 0.1346 Accuracy 0.9618\n",
      "Time taken for 1 epoch: 158.4544882774353 secs\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0907 Accuracy 0.9761\n",
      "Epoch 117 Batch 50 Loss 0.1411 Accuracy 0.9575\n",
      "Epoch 117 Batch 100 Loss 0.1432 Accuracy 0.9577\n",
      "Epoch 117 Batch 150 Loss 0.1395 Accuracy 0.9587\n",
      "Epoch 117 Batch 200 Loss 0.1355 Accuracy 0.9601\n",
      "Epoch 117 Batch 250 Loss 0.1323 Accuracy 0.9615\n",
      "Epoch 117 Batch 300 Loss 0.1313 Accuracy 0.9621\n",
      "Epoch 117 Batch 350 Loss 0.1302 Accuracy 0.9626\n",
      "Epoch 117 Batch 400 Loss 0.1310 Accuracy 0.9627\n",
      "Epoch 117 Batch 450 Loss 0.1315 Accuracy 0.9626\n",
      "Epoch 117 Batch 500 Loss 0.1328 Accuracy 0.9623\n",
      "Epoch 117 Loss 0.1336 Accuracy 0.9621\n",
      "Time taken for 1 epoch: 157.72347927093506 secs\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.1176 Accuracy 0.9697\n",
      "Epoch 118 Batch 50 Loss 0.1327 Accuracy 0.9612\n",
      "Epoch 118 Batch 100 Loss 0.1380 Accuracy 0.9602\n",
      "Epoch 118 Batch 150 Loss 0.1352 Accuracy 0.9609\n",
      "Epoch 118 Batch 200 Loss 0.1338 Accuracy 0.9616\n",
      "Epoch 118 Batch 250 Loss 0.1321 Accuracy 0.9622\n",
      "Epoch 118 Batch 300 Loss 0.1305 Accuracy 0.9626\n",
      "Epoch 118 Batch 350 Loss 0.1306 Accuracy 0.9628\n",
      "Epoch 118 Batch 400 Loss 0.1314 Accuracy 0.9625\n",
      "Epoch 118 Batch 450 Loss 0.1318 Accuracy 0.9626\n",
      "Epoch 118 Batch 500 Loss 0.1316 Accuracy 0.9629\n",
      "Epoch 118 Loss 0.1319 Accuracy 0.9629\n",
      "Time taken for 1 epoch: 158.33400702476501 secs\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.1330 Accuracy 0.9627\n",
      "Epoch 119 Batch 50 Loss 0.1453 Accuracy 0.9570\n",
      "Epoch 119 Batch 100 Loss 0.1434 Accuracy 0.9577\n",
      "Epoch 119 Batch 150 Loss 0.1380 Accuracy 0.9595\n",
      "Epoch 119 Batch 200 Loss 0.1331 Accuracy 0.9613\n",
      "Epoch 119 Batch 250 Loss 0.1294 Accuracy 0.9623\n",
      "Epoch 119 Batch 300 Loss 0.1293 Accuracy 0.9625\n",
      "Epoch 119 Batch 350 Loss 0.1292 Accuracy 0.9627\n",
      "Epoch 119 Batch 400 Loss 0.1295 Accuracy 0.9629\n",
      "Epoch 119 Batch 450 Loss 0.1294 Accuracy 0.9633\n",
      "Epoch 119 Batch 500 Loss 0.1311 Accuracy 0.9629\n",
      "Epoch 119 Loss 0.1316 Accuracy 0.9628\n",
      "Time taken for 1 epoch: 158.424218416214 secs\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.1403 Accuracy 0.9675\n",
      "Epoch 120 Batch 50 Loss 0.1420 Accuracy 0.9593\n",
      "Epoch 120 Batch 100 Loss 0.1399 Accuracy 0.9596\n",
      "Epoch 120 Batch 150 Loss 0.1355 Accuracy 0.9606\n",
      "Epoch 120 Batch 200 Loss 0.1319 Accuracy 0.9619\n",
      "Epoch 120 Batch 250 Loss 0.1281 Accuracy 0.9632\n",
      "Epoch 120 Batch 300 Loss 0.1255 Accuracy 0.9641\n",
      "Epoch 120 Batch 350 Loss 0.1253 Accuracy 0.9644\n",
      "Epoch 120 Batch 400 Loss 0.1265 Accuracy 0.9641\n",
      "Epoch 120 Batch 450 Loss 0.1270 Accuracy 0.9640\n",
      "Epoch 120 Batch 500 Loss 0.1286 Accuracy 0.9636\n",
      "Saving checkpoint for epoch 120 at ./checkpoints/train\\ckpt-24\n",
      "Epoch 120 Loss 0.1292 Accuracy 0.9636\n",
      "Time taken for 1 epoch: 165.75353860855103 secs\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.1596 Accuracy 0.9381\n",
      "Epoch 121 Batch 50 Loss 0.1332 Accuracy 0.9608\n",
      "Epoch 121 Batch 100 Loss 0.1334 Accuracy 0.9608\n",
      "Epoch 121 Batch 150 Loss 0.1295 Accuracy 0.9620\n",
      "Epoch 121 Batch 200 Loss 0.1281 Accuracy 0.9625\n",
      "Epoch 121 Batch 250 Loss 0.1270 Accuracy 0.9634\n",
      "Epoch 121 Batch 300 Loss 0.1265 Accuracy 0.9638\n",
      "Epoch 121 Batch 350 Loss 0.1272 Accuracy 0.9635\n",
      "Epoch 121 Batch 400 Loss 0.1270 Accuracy 0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121 Batch 450 Loss 0.1279 Accuracy 0.9633\n",
      "Epoch 121 Batch 500 Loss 0.1294 Accuracy 0.9631\n",
      "Epoch 121 Loss 0.1299 Accuracy 0.9630\n",
      "Time taken for 1 epoch: 172.97605085372925 secs\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.1140 Accuracy 0.9592\n",
      "Epoch 122 Batch 50 Loss 0.1335 Accuracy 0.9589\n",
      "Epoch 122 Batch 100 Loss 0.1379 Accuracy 0.9590\n",
      "Epoch 122 Batch 150 Loss 0.1341 Accuracy 0.9605\n",
      "Epoch 122 Batch 200 Loss 0.1291 Accuracy 0.9624\n",
      "Epoch 122 Batch 250 Loss 0.1276 Accuracy 0.9633\n",
      "Epoch 122 Batch 300 Loss 0.1254 Accuracy 0.9637\n",
      "Epoch 122 Batch 350 Loss 0.1260 Accuracy 0.9638\n",
      "Epoch 122 Batch 400 Loss 0.1257 Accuracy 0.9642\n",
      "Epoch 122 Batch 450 Loss 0.1263 Accuracy 0.9642\n",
      "Epoch 122 Batch 500 Loss 0.1270 Accuracy 0.9640\n",
      "Epoch 122 Loss 0.1276 Accuracy 0.9639\n",
      "Time taken for 1 epoch: 179.40934443473816 secs\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.1814 Accuracy 0.9558\n",
      "Epoch 123 Batch 50 Loss 0.1338 Accuracy 0.9605\n",
      "Epoch 123 Batch 100 Loss 0.1369 Accuracy 0.9607\n",
      "Epoch 123 Batch 150 Loss 0.1328 Accuracy 0.9616\n",
      "Epoch 123 Batch 200 Loss 0.1293 Accuracy 0.9629\n",
      "Epoch 123 Batch 250 Loss 0.1269 Accuracy 0.9639\n",
      "Epoch 123 Batch 300 Loss 0.1243 Accuracy 0.9646\n",
      "Epoch 123 Batch 350 Loss 0.1244 Accuracy 0.9645\n",
      "Epoch 123 Batch 400 Loss 0.1250 Accuracy 0.9645\n",
      "Epoch 123 Batch 450 Loss 0.1261 Accuracy 0.9644\n",
      "Epoch 123 Batch 500 Loss 0.1267 Accuracy 0.9643\n",
      "Epoch 123 Loss 0.1268 Accuracy 0.9644\n",
      "Time taken for 1 epoch: 179.58879923820496 secs\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0902 Accuracy 0.9765\n",
      "Epoch 124 Batch 50 Loss 0.1312 Accuracy 0.9605\n",
      "Epoch 124 Batch 100 Loss 0.1331 Accuracy 0.9615\n",
      "Epoch 124 Batch 150 Loss 0.1312 Accuracy 0.9619\n",
      "Epoch 124 Batch 200 Loss 0.1281 Accuracy 0.9631\n",
      "Epoch 124 Batch 250 Loss 0.1267 Accuracy 0.9636\n",
      "Epoch 124 Batch 300 Loss 0.1252 Accuracy 0.9641\n",
      "Epoch 124 Batch 350 Loss 0.1250 Accuracy 0.9642\n",
      "Epoch 124 Batch 400 Loss 0.1260 Accuracy 0.9641\n",
      "Epoch 124 Batch 450 Loss 0.1263 Accuracy 0.9641\n",
      "Epoch 124 Batch 500 Loss 0.1265 Accuracy 0.9641\n",
      "Epoch 124 Loss 0.1273 Accuracy 0.9639\n",
      "Time taken for 1 epoch: 155.75899600982666 secs\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.1692 Accuracy 0.9608\n",
      "Epoch 125 Batch 50 Loss 0.1242 Accuracy 0.9649\n",
      "Epoch 125 Batch 100 Loss 0.1329 Accuracy 0.9615\n",
      "Epoch 125 Batch 150 Loss 0.1280 Accuracy 0.9631\n",
      "Epoch 125 Batch 200 Loss 0.1279 Accuracy 0.9633\n",
      "Epoch 125 Batch 250 Loss 0.1260 Accuracy 0.9639\n",
      "Epoch 125 Batch 300 Loss 0.1249 Accuracy 0.9642\n",
      "Epoch 125 Batch 350 Loss 0.1248 Accuracy 0.9641\n",
      "Epoch 125 Batch 400 Loss 0.1251 Accuracy 0.9644\n",
      "Epoch 125 Batch 450 Loss 0.1257 Accuracy 0.9644\n",
      "Epoch 125 Batch 500 Loss 0.1263 Accuracy 0.9643\n",
      "Saving checkpoint for epoch 125 at ./checkpoints/train\\ckpt-25\n",
      "Epoch 125 Loss 0.1273 Accuracy 0.9641\n",
      "Time taken for 1 epoch: 166.6179780960083 secs\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.1131 Accuracy 0.9662\n",
      "Epoch 126 Batch 50 Loss 0.1298 Accuracy 0.9618\n",
      "Epoch 126 Batch 100 Loss 0.1311 Accuracy 0.9624\n",
      "Epoch 126 Batch 150 Loss 0.1287 Accuracy 0.9626\n",
      "Epoch 126 Batch 200 Loss 0.1260 Accuracy 0.9634\n",
      "Epoch 126 Batch 250 Loss 0.1226 Accuracy 0.9644\n",
      "Epoch 126 Batch 300 Loss 0.1220 Accuracy 0.9646\n",
      "Epoch 126 Batch 350 Loss 0.1235 Accuracy 0.9645\n",
      "Epoch 126 Batch 400 Loss 0.1241 Accuracy 0.9646\n",
      "Epoch 126 Batch 450 Loss 0.1248 Accuracy 0.9646\n",
      "Epoch 126 Batch 500 Loss 0.1249 Accuracy 0.9647\n",
      "Epoch 126 Loss 0.1251 Accuracy 0.9647\n",
      "Time taken for 1 epoch: 157.8790967464447 secs\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.1523 Accuracy 0.9536\n",
      "Epoch 127 Batch 50 Loss 0.1380 Accuracy 0.9604\n",
      "Epoch 127 Batch 100 Loss 0.1331 Accuracy 0.9618\n",
      "Epoch 127 Batch 150 Loss 0.1290 Accuracy 0.9631\n",
      "Epoch 127 Batch 200 Loss 0.1271 Accuracy 0.9636\n",
      "Epoch 127 Batch 250 Loss 0.1229 Accuracy 0.9651\n",
      "Epoch 127 Batch 300 Loss 0.1209 Accuracy 0.9657\n",
      "Epoch 127 Batch 350 Loss 0.1207 Accuracy 0.9657\n",
      "Epoch 127 Batch 400 Loss 0.1211 Accuracy 0.9657\n",
      "Epoch 127 Batch 450 Loss 0.1221 Accuracy 0.9656\n",
      "Epoch 127 Batch 500 Loss 0.1229 Accuracy 0.9655\n",
      "Epoch 127 Loss 0.1233 Accuracy 0.9653\n",
      "Time taken for 1 epoch: 160.9966585636139 secs\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.1461 Accuracy 0.9539\n",
      "Epoch 128 Batch 50 Loss 0.1294 Accuracy 0.9601\n",
      "Epoch 128 Batch 100 Loss 0.1279 Accuracy 0.9616\n",
      "Epoch 128 Batch 150 Loss 0.1280 Accuracy 0.9612\n",
      "Epoch 128 Batch 200 Loss 0.1247 Accuracy 0.9628\n",
      "Epoch 128 Batch 250 Loss 0.1229 Accuracy 0.9638\n",
      "Epoch 128 Batch 300 Loss 0.1224 Accuracy 0.9642\n",
      "Epoch 128 Batch 350 Loss 0.1231 Accuracy 0.9645\n",
      "Epoch 128 Batch 400 Loss 0.1233 Accuracy 0.9647\n",
      "Epoch 128 Batch 450 Loss 0.1240 Accuracy 0.9648\n",
      "Epoch 128 Batch 500 Loss 0.1244 Accuracy 0.9648\n",
      "Epoch 128 Loss 0.1248 Accuracy 0.9646\n",
      "Time taken for 1 epoch: 162.57321310043335 secs\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.1399 Accuracy 0.9539\n",
      "Epoch 129 Batch 50 Loss 0.1375 Accuracy 0.9591\n",
      "Epoch 129 Batch 100 Loss 0.1366 Accuracy 0.9593\n",
      "Epoch 129 Batch 150 Loss 0.1317 Accuracy 0.9608\n",
      "Epoch 129 Batch 200 Loss 0.1266 Accuracy 0.9624\n",
      "Epoch 129 Batch 250 Loss 0.1229 Accuracy 0.9639\n",
      "Epoch 129 Batch 300 Loss 0.1219 Accuracy 0.9647\n",
      "Epoch 129 Batch 350 Loss 0.1223 Accuracy 0.9647\n",
      "Epoch 129 Batch 400 Loss 0.1222 Accuracy 0.9651\n",
      "Epoch 129 Batch 450 Loss 0.1218 Accuracy 0.9653\n",
      "Epoch 129 Batch 500 Loss 0.1228 Accuracy 0.9653\n",
      "Epoch 129 Loss 0.1227 Accuracy 0.9653\n",
      "Time taken for 1 epoch: 163.09978127479553 secs\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0640 Accuracy 0.9773\n",
      "Epoch 130 Batch 50 Loss 0.1293 Accuracy 0.9616\n",
      "Epoch 130 Batch 100 Loss 0.1297 Accuracy 0.9621\n",
      "Epoch 130 Batch 150 Loss 0.1243 Accuracy 0.9640\n",
      "Epoch 130 Batch 200 Loss 0.1195 Accuracy 0.9655\n",
      "Epoch 130 Batch 250 Loss 0.1167 Accuracy 0.9663\n",
      "Epoch 130 Batch 300 Loss 0.1162 Accuracy 0.9665\n",
      "Epoch 130 Batch 350 Loss 0.1177 Accuracy 0.9663\n",
      "Epoch 130 Batch 400 Loss 0.1186 Accuracy 0.9661\n",
      "Epoch 130 Batch 450 Loss 0.1187 Accuracy 0.9662\n",
      "Epoch 130 Batch 500 Loss 0.1197 Accuracy 0.9660\n",
      "Saving checkpoint for epoch 130 at ./checkpoints/train\\ckpt-26\n",
      "Epoch 130 Loss 0.1199 Accuracy 0.9660\n",
      "Time taken for 1 epoch: 169.4287073612213 secs\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.1658 Accuracy 0.9492\n",
      "Epoch 131 Batch 50 Loss 0.1328 Accuracy 0.9609\n",
      "Epoch 131 Batch 100 Loss 0.1308 Accuracy 0.9613\n",
      "Epoch 131 Batch 150 Loss 0.1272 Accuracy 0.9628\n",
      "Epoch 131 Batch 200 Loss 0.1262 Accuracy 0.9635\n",
      "Epoch 131 Batch 250 Loss 0.1219 Accuracy 0.9650\n",
      "Epoch 131 Batch 300 Loss 0.1209 Accuracy 0.9652\n",
      "Epoch 131 Batch 350 Loss 0.1208 Accuracy 0.9655\n",
      "Epoch 131 Batch 400 Loss 0.1210 Accuracy 0.9655\n",
      "Epoch 131 Batch 450 Loss 0.1217 Accuracy 0.9655\n",
      "Epoch 131 Batch 500 Loss 0.1213 Accuracy 0.9659\n",
      "Epoch 131 Loss 0.1216 Accuracy 0.9659\n",
      "Time taken for 1 epoch: 162.74410128593445 secs\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.1365 Accuracy 0.9655\n",
      "Epoch 132 Batch 50 Loss 0.1351 Accuracy 0.9614\n",
      "Epoch 132 Batch 100 Loss 0.1320 Accuracy 0.9620\n",
      "Epoch 132 Batch 150 Loss 0.1278 Accuracy 0.9635\n",
      "Epoch 132 Batch 200 Loss 0.1218 Accuracy 0.9653\n",
      "Epoch 132 Batch 250 Loss 0.1185 Accuracy 0.9664\n",
      "Epoch 132 Batch 300 Loss 0.1179 Accuracy 0.9666\n",
      "Epoch 132 Batch 350 Loss 0.1184 Accuracy 0.9667\n",
      "Epoch 132 Batch 400 Loss 0.1189 Accuracy 0.9666\n",
      "Epoch 132 Batch 450 Loss 0.1201 Accuracy 0.9664\n",
      "Epoch 132 Batch 500 Loss 0.1206 Accuracy 0.9663\n",
      "Epoch 132 Loss 0.1207 Accuracy 0.9663\n",
      "Time taken for 1 epoch: 162.80328679084778 secs\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.1176 Accuracy 0.9630\n",
      "Epoch 133 Batch 50 Loss 0.1383 Accuracy 0.9595\n",
      "Epoch 133 Batch 100 Loss 0.1308 Accuracy 0.9628\n",
      "Epoch 133 Batch 150 Loss 0.1269 Accuracy 0.9640\n",
      "Epoch 133 Batch 200 Loss 0.1219 Accuracy 0.9655\n",
      "Epoch 133 Batch 250 Loss 0.1199 Accuracy 0.9662\n",
      "Epoch 133 Batch 300 Loss 0.1183 Accuracy 0.9666\n",
      "Epoch 133 Batch 350 Loss 0.1175 Accuracy 0.9670\n",
      "Epoch 133 Batch 400 Loss 0.1183 Accuracy 0.9667\n",
      "Epoch 133 Batch 450 Loss 0.1198 Accuracy 0.9663\n",
      "Epoch 133 Batch 500 Loss 0.1192 Accuracy 0.9666\n",
      "Epoch 133 Loss 0.1195 Accuracy 0.9666\n",
      "Time taken for 1 epoch: 161.38555908203125 secs\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0924 Accuracy 0.9700\n",
      "Epoch 134 Batch 50 Loss 0.1274 Accuracy 0.9626\n",
      "Epoch 134 Batch 100 Loss 0.1261 Accuracy 0.9637\n",
      "Epoch 134 Batch 150 Loss 0.1248 Accuracy 0.9644\n",
      "Epoch 134 Batch 200 Loss 0.1229 Accuracy 0.9648\n",
      "Epoch 134 Batch 250 Loss 0.1195 Accuracy 0.9659\n",
      "Epoch 134 Batch 300 Loss 0.1182 Accuracy 0.9664\n",
      "Epoch 134 Batch 350 Loss 0.1182 Accuracy 0.9665\n",
      "Epoch 134 Batch 400 Loss 0.1187 Accuracy 0.9664\n",
      "Epoch 134 Batch 450 Loss 0.1190 Accuracy 0.9663\n",
      "Epoch 134 Batch 500 Loss 0.1204 Accuracy 0.9662\n",
      "Epoch 134 Loss 0.1205 Accuracy 0.9661\n",
      "Time taken for 1 epoch: 162.84003138542175 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135 Batch 0 Loss 0.1784 Accuracy 0.9331\n",
      "Epoch 135 Batch 50 Loss 0.1244 Accuracy 0.9631\n",
      "Epoch 135 Batch 100 Loss 0.1211 Accuracy 0.9650\n",
      "Epoch 135 Batch 150 Loss 0.1205 Accuracy 0.9653\n",
      "Epoch 135 Batch 200 Loss 0.1194 Accuracy 0.9659\n",
      "Epoch 135 Batch 250 Loss 0.1163 Accuracy 0.9666\n",
      "Epoch 135 Batch 300 Loss 0.1160 Accuracy 0.9670\n",
      "Epoch 135 Batch 350 Loss 0.1161 Accuracy 0.9671\n",
      "Epoch 135 Batch 400 Loss 0.1177 Accuracy 0.9669\n",
      "Epoch 135 Batch 450 Loss 0.1184 Accuracy 0.9667\n",
      "Epoch 135 Batch 500 Loss 0.1190 Accuracy 0.9666\n",
      "Saving checkpoint for epoch 135 at ./checkpoints/train\\ckpt-27\n",
      "Epoch 135 Loss 0.1193 Accuracy 0.9666\n",
      "Time taken for 1 epoch: 165.35190343856812 secs\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.1550 Accuracy 0.9609\n",
      "Epoch 136 Batch 50 Loss 0.1295 Accuracy 0.9633\n",
      "Epoch 136 Batch 100 Loss 0.1262 Accuracy 0.9640\n",
      "Epoch 136 Batch 150 Loss 0.1228 Accuracy 0.9648\n",
      "Epoch 136 Batch 200 Loss 0.1193 Accuracy 0.9658\n",
      "Epoch 136 Batch 250 Loss 0.1160 Accuracy 0.9670\n",
      "Epoch 136 Batch 300 Loss 0.1165 Accuracy 0.9670\n",
      "Epoch 136 Batch 350 Loss 0.1175 Accuracy 0.9670\n",
      "Epoch 136 Batch 400 Loss 0.1172 Accuracy 0.9670\n",
      "Epoch 136 Batch 450 Loss 0.1181 Accuracy 0.9670\n",
      "Epoch 136 Batch 500 Loss 0.1180 Accuracy 0.9670\n",
      "Epoch 136 Loss 0.1183 Accuracy 0.9670\n",
      "Time taken for 1 epoch: 158.0458025932312 secs\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.1381 Accuracy 0.9532\n",
      "Epoch 137 Batch 50 Loss 0.1319 Accuracy 0.9605\n",
      "Epoch 137 Batch 100 Loss 0.1320 Accuracy 0.9611\n",
      "Epoch 137 Batch 150 Loss 0.1264 Accuracy 0.9629\n",
      "Epoch 137 Batch 200 Loss 0.1197 Accuracy 0.9647\n",
      "Epoch 137 Batch 250 Loss 0.1173 Accuracy 0.9656\n",
      "Epoch 137 Batch 300 Loss 0.1150 Accuracy 0.9666\n",
      "Epoch 137 Batch 350 Loss 0.1133 Accuracy 0.9674\n",
      "Epoch 137 Batch 400 Loss 0.1144 Accuracy 0.9672\n",
      "Epoch 137 Batch 450 Loss 0.1156 Accuracy 0.9670\n",
      "Epoch 137 Batch 500 Loss 0.1174 Accuracy 0.9666\n",
      "Epoch 137 Loss 0.1178 Accuracy 0.9665\n",
      "Time taken for 1 epoch: 157.2462043762207 secs\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0904 Accuracy 0.9674\n",
      "Epoch 138 Batch 50 Loss 0.1351 Accuracy 0.9603\n",
      "Epoch 138 Batch 100 Loss 0.1276 Accuracy 0.9628\n",
      "Epoch 138 Batch 150 Loss 0.1196 Accuracy 0.9653\n",
      "Epoch 138 Batch 200 Loss 0.1170 Accuracy 0.9665\n",
      "Epoch 138 Batch 250 Loss 0.1145 Accuracy 0.9675\n",
      "Epoch 138 Batch 300 Loss 0.1136 Accuracy 0.9678\n",
      "Epoch 138 Batch 350 Loss 0.1147 Accuracy 0.9676\n",
      "Epoch 138 Batch 400 Loss 0.1145 Accuracy 0.9678\n",
      "Epoch 138 Batch 450 Loss 0.1142 Accuracy 0.9679\n",
      "Epoch 138 Batch 500 Loss 0.1146 Accuracy 0.9678\n",
      "Epoch 138 Loss 0.1149 Accuracy 0.9677\n",
      "Time taken for 1 epoch: 153.88926792144775 secs\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.1042 Accuracy 0.9648\n",
      "Epoch 139 Batch 50 Loss 0.1318 Accuracy 0.9611\n",
      "Epoch 139 Batch 100 Loss 0.1285 Accuracy 0.9626\n",
      "Epoch 139 Batch 150 Loss 0.1220 Accuracy 0.9643\n",
      "Epoch 139 Batch 200 Loss 0.1184 Accuracy 0.9655\n",
      "Epoch 139 Batch 250 Loss 0.1167 Accuracy 0.9663\n",
      "Epoch 139 Batch 300 Loss 0.1158 Accuracy 0.9666\n",
      "Epoch 139 Batch 350 Loss 0.1151 Accuracy 0.9669\n",
      "Epoch 139 Batch 400 Loss 0.1160 Accuracy 0.9669\n",
      "Epoch 139 Batch 450 Loss 0.1164 Accuracy 0.9668\n",
      "Epoch 139 Batch 500 Loss 0.1171 Accuracy 0.9668\n",
      "Epoch 139 Loss 0.1170 Accuracy 0.9668\n",
      "Time taken for 1 epoch: 153.983384847641 secs\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.1302 Accuracy 0.9536\n",
      "Epoch 140 Batch 50 Loss 0.1226 Accuracy 0.9628\n",
      "Epoch 140 Batch 100 Loss 0.1235 Accuracy 0.9635\n",
      "Epoch 140 Batch 150 Loss 0.1233 Accuracy 0.9638\n",
      "Epoch 140 Batch 200 Loss 0.1189 Accuracy 0.9653\n",
      "Epoch 140 Batch 250 Loss 0.1153 Accuracy 0.9665\n",
      "Epoch 140 Batch 300 Loss 0.1138 Accuracy 0.9674\n",
      "Epoch 140 Batch 350 Loss 0.1132 Accuracy 0.9678\n",
      "Epoch 140 Batch 400 Loss 0.1140 Accuracy 0.9677\n",
      "Epoch 140 Batch 450 Loss 0.1146 Accuracy 0.9679\n",
      "Epoch 140 Batch 500 Loss 0.1150 Accuracy 0.9677\n",
      "Saving checkpoint for epoch 140 at ./checkpoints/train\\ckpt-28\n",
      "Epoch 140 Loss 0.1156 Accuracy 0.9675\n",
      "Time taken for 1 epoch: 159.9997103214264 secs\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0690 Accuracy 0.9829\n",
      "Epoch 141 Batch 50 Loss 0.1212 Accuracy 0.9628\n",
      "Epoch 141 Batch 100 Loss 0.1191 Accuracy 0.9648\n",
      "Epoch 141 Batch 150 Loss 0.1178 Accuracy 0.9653\n",
      "Epoch 141 Batch 200 Loss 0.1161 Accuracy 0.9661\n",
      "Epoch 141 Batch 250 Loss 0.1145 Accuracy 0.9666\n",
      "Epoch 141 Batch 300 Loss 0.1131 Accuracy 0.9672\n",
      "Epoch 141 Batch 350 Loss 0.1127 Accuracy 0.9678\n",
      "Epoch 141 Batch 400 Loss 0.1150 Accuracy 0.9673\n",
      "Epoch 141 Batch 450 Loss 0.1139 Accuracy 0.9676\n",
      "Epoch 141 Batch 500 Loss 0.1147 Accuracy 0.9674\n",
      "Epoch 141 Loss 0.1149 Accuracy 0.9674\n",
      "Time taken for 1 epoch: 153.44931292533875 secs\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.1573 Accuracy 0.9549\n",
      "Epoch 142 Batch 50 Loss 0.1206 Accuracy 0.9643\n",
      "Epoch 142 Batch 100 Loss 0.1188 Accuracy 0.9646\n",
      "Epoch 142 Batch 150 Loss 0.1140 Accuracy 0.9667\n",
      "Epoch 142 Batch 200 Loss 0.1127 Accuracy 0.9675\n",
      "Epoch 142 Batch 250 Loss 0.1123 Accuracy 0.9679\n",
      "Epoch 142 Batch 300 Loss 0.1116 Accuracy 0.9683\n",
      "Epoch 142 Batch 350 Loss 0.1111 Accuracy 0.9684\n",
      "Epoch 142 Batch 400 Loss 0.1113 Accuracy 0.9685\n",
      "Epoch 142 Batch 450 Loss 0.1125 Accuracy 0.9683\n",
      "Epoch 142 Batch 500 Loss 0.1130 Accuracy 0.9682\n",
      "Epoch 142 Loss 0.1138 Accuracy 0.9681\n",
      "Time taken for 1 epoch: 154.36855697631836 secs\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0978 Accuracy 0.9720\n",
      "Epoch 143 Batch 50 Loss 0.1209 Accuracy 0.9646\n",
      "Epoch 143 Batch 100 Loss 0.1222 Accuracy 0.9644\n",
      "Epoch 143 Batch 150 Loss 0.1201 Accuracy 0.9647\n",
      "Epoch 143 Batch 200 Loss 0.1167 Accuracy 0.9661\n",
      "Epoch 143 Batch 250 Loss 0.1131 Accuracy 0.9671\n",
      "Epoch 143 Batch 300 Loss 0.1112 Accuracy 0.9677\n",
      "Epoch 143 Batch 350 Loss 0.1114 Accuracy 0.9676\n",
      "Epoch 143 Batch 400 Loss 0.1123 Accuracy 0.9675\n",
      "Epoch 143 Batch 450 Loss 0.1127 Accuracy 0.9675\n",
      "Epoch 143 Batch 500 Loss 0.1132 Accuracy 0.9675\n",
      "Epoch 143 Loss 0.1135 Accuracy 0.9674\n",
      "Time taken for 1 epoch: 153.06097388267517 secs\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.1158 Accuracy 0.9661\n",
      "Epoch 144 Batch 50 Loss 0.1164 Accuracy 0.9669\n",
      "Epoch 144 Batch 100 Loss 0.1177 Accuracy 0.9667\n",
      "Epoch 144 Batch 150 Loss 0.1189 Accuracy 0.9666\n",
      "Epoch 144 Batch 200 Loss 0.1147 Accuracy 0.9674\n",
      "Epoch 144 Batch 250 Loss 0.1130 Accuracy 0.9682\n",
      "Epoch 144 Batch 300 Loss 0.1117 Accuracy 0.9686\n",
      "Epoch 144 Batch 350 Loss 0.1117 Accuracy 0.9686\n",
      "Epoch 144 Batch 400 Loss 0.1116 Accuracy 0.9686\n",
      "Epoch 144 Batch 450 Loss 0.1132 Accuracy 0.9682\n",
      "Epoch 144 Batch 500 Loss 0.1129 Accuracy 0.9682\n",
      "Epoch 144 Loss 0.1132 Accuracy 0.9681\n",
      "Time taken for 1 epoch: 153.29398488998413 secs\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0919 Accuracy 0.9771\n",
      "Epoch 145 Batch 50 Loss 0.1135 Accuracy 0.9655\n",
      "Epoch 145 Batch 100 Loss 0.1158 Accuracy 0.9656\n",
      "Epoch 145 Batch 150 Loss 0.1131 Accuracy 0.9668\n",
      "Epoch 145 Batch 200 Loss 0.1120 Accuracy 0.9676\n",
      "Epoch 145 Batch 250 Loss 0.1087 Accuracy 0.9688\n",
      "Epoch 145 Batch 300 Loss 0.1078 Accuracy 0.9689\n",
      "Epoch 145 Batch 350 Loss 0.1093 Accuracy 0.9690\n",
      "Epoch 145 Batch 400 Loss 0.1098 Accuracy 0.9690\n",
      "Epoch 145 Batch 450 Loss 0.1108 Accuracy 0.9689\n",
      "Epoch 145 Batch 500 Loss 0.1125 Accuracy 0.9685\n",
      "Saving checkpoint for epoch 145 at ./checkpoints/train\\ckpt-29\n",
      "Epoch 145 Loss 0.1128 Accuracy 0.9685\n",
      "Time taken for 1 epoch: 160.08377838134766 secs\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0826 Accuracy 0.9898\n",
      "Epoch 146 Batch 50 Loss 0.1209 Accuracy 0.9658\n",
      "Epoch 146 Batch 100 Loss 0.1183 Accuracy 0.9661\n",
      "Epoch 146 Batch 150 Loss 0.1148 Accuracy 0.9669\n",
      "Epoch 146 Batch 200 Loss 0.1114 Accuracy 0.9680\n",
      "Epoch 146 Batch 250 Loss 0.1098 Accuracy 0.9686\n",
      "Epoch 146 Batch 300 Loss 0.1093 Accuracy 0.9687\n",
      "Epoch 146 Batch 350 Loss 0.1096 Accuracy 0.9689\n",
      "Epoch 146 Batch 400 Loss 0.1106 Accuracy 0.9686\n",
      "Epoch 146 Batch 450 Loss 0.1117 Accuracy 0.9684\n",
      "Epoch 146 Batch 500 Loss 0.1129 Accuracy 0.9681\n",
      "Epoch 146 Loss 0.1133 Accuracy 0.9681\n",
      "Time taken for 1 epoch: 153.84061932563782 secs\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.1040 Accuracy 0.9611\n",
      "Epoch 147 Batch 50 Loss 0.1159 Accuracy 0.9657\n",
      "Epoch 147 Batch 100 Loss 0.1122 Accuracy 0.9665\n",
      "Epoch 147 Batch 150 Loss 0.1118 Accuracy 0.9671\n",
      "Epoch 147 Batch 200 Loss 0.1101 Accuracy 0.9677\n",
      "Epoch 147 Batch 250 Loss 0.1085 Accuracy 0.9687\n",
      "Epoch 147 Batch 300 Loss 0.1091 Accuracy 0.9688\n",
      "Epoch 147 Batch 350 Loss 0.1091 Accuracy 0.9688\n",
      "Epoch 147 Batch 400 Loss 0.1090 Accuracy 0.9689\n",
      "Epoch 147 Batch 450 Loss 0.1087 Accuracy 0.9691\n",
      "Epoch 147 Batch 500 Loss 0.1096 Accuracy 0.9689\n",
      "Epoch 147 Loss 0.1099 Accuracy 0.9689\n",
      "Time taken for 1 epoch: 153.68253135681152 secs\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.1063 Accuracy 0.9655\n",
      "Epoch 148 Batch 50 Loss 0.1278 Accuracy 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148 Batch 100 Loss 0.1197 Accuracy 0.9654\n",
      "Epoch 148 Batch 150 Loss 0.1163 Accuracy 0.9662\n",
      "Epoch 148 Batch 200 Loss 0.1125 Accuracy 0.9675\n",
      "Epoch 148 Batch 250 Loss 0.1100 Accuracy 0.9686\n",
      "Epoch 148 Batch 300 Loss 0.1098 Accuracy 0.9687\n",
      "Epoch 148 Batch 350 Loss 0.1088 Accuracy 0.9690\n",
      "Epoch 148 Batch 400 Loss 0.1080 Accuracy 0.9693\n",
      "Epoch 148 Batch 450 Loss 0.1087 Accuracy 0.9693\n",
      "Epoch 148 Batch 500 Loss 0.1097 Accuracy 0.9691\n",
      "Epoch 148 Loss 0.1103 Accuracy 0.9689\n",
      "Time taken for 1 epoch: 154.21997618675232 secs\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.1280 Accuracy 0.9582\n",
      "Epoch 149 Batch 50 Loss 0.1195 Accuracy 0.9657\n",
      "Epoch 149 Batch 100 Loss 0.1200 Accuracy 0.9645\n",
      "Epoch 149 Batch 150 Loss 0.1169 Accuracy 0.9657\n",
      "Epoch 149 Batch 200 Loss 0.1134 Accuracy 0.9669\n",
      "Epoch 149 Batch 250 Loss 0.1114 Accuracy 0.9676\n",
      "Epoch 149 Batch 300 Loss 0.1101 Accuracy 0.9683\n",
      "Epoch 149 Batch 350 Loss 0.1093 Accuracy 0.9688\n",
      "Epoch 149 Batch 400 Loss 0.1095 Accuracy 0.9689\n",
      "Epoch 149 Batch 450 Loss 0.1101 Accuracy 0.9688\n",
      "Epoch 149 Batch 500 Loss 0.1112 Accuracy 0.9686\n",
      "Epoch 149 Loss 0.1114 Accuracy 0.9686\n",
      "Time taken for 1 epoch: 153.52258610725403 secs\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.1018 Accuracy 0.9635\n",
      "Epoch 150 Batch 50 Loss 0.1183 Accuracy 0.9644\n",
      "Epoch 150 Batch 100 Loss 0.1140 Accuracy 0.9660\n",
      "Epoch 150 Batch 150 Loss 0.1117 Accuracy 0.9671\n",
      "Epoch 150 Batch 200 Loss 0.1093 Accuracy 0.9680\n",
      "Epoch 150 Batch 250 Loss 0.1076 Accuracy 0.9684\n",
      "Epoch 150 Batch 300 Loss 0.1062 Accuracy 0.9692\n",
      "Epoch 150 Batch 350 Loss 0.1074 Accuracy 0.9690\n",
      "Epoch 150 Batch 400 Loss 0.1083 Accuracy 0.9689\n",
      "Epoch 150 Batch 450 Loss 0.1096 Accuracy 0.9686\n",
      "Epoch 150 Batch 500 Loss 0.1100 Accuracy 0.9687\n",
      "Saving checkpoint for epoch 150 at ./checkpoints/train\\ckpt-30\n",
      "Epoch 150 Loss 0.1100 Accuracy 0.9687\n",
      "Time taken for 1 epoch: 160.3680820465088 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:21:08.171405Z",
     "iopub.status.busy": "2021-01-08T06:21:08.170518Z",
     "iopub.status.idle": "2021-01-08T06:21:08.172899Z",
     "shell.execute_reply": "2021-01-08T06:21:08.172443Z"
    },
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:21:08.180529Z",
     "iopub.status.busy": "2021-01-08T06:21:08.179728Z",
     "iopub.status.idle": "2021-01-08T06:21:08.181979Z",
     "shell.execute_reply": "2021-01-08T06:21:08.181554Z"
    },
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:21:08.186861Z",
     "iopub.status.busy": "2021-01-08T06:21:08.186040Z",
     "iopub.status.idle": "2021-01-08T06:21:08.188352Z",
     "shell.execute_reply": "2021-01-08T06:21:08.187946Z"
    },
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result.numpy()\n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submision(descriptions):\n",
    "    f = open(\"myfile.txt\", \"w\")\n",
    "    f.write(\"name\\n\")\n",
    "    \n",
    "    for sentence in descriptions:\n",
    "    \n",
    "        result, attention_weights = evaluate(sentence)\n",
    "        predicted_sentence = tokenizer_en.decode([i for i in result.numpy()\n",
    "                                                if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    \n",
    "        f.write(predicted_sentence + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = pd.read_csv(\"test_descriptions.csv\")\n",
    "\n",
    "lines = test.values.ravel()\n",
    "\n",
    "generate_submision(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (6,) and (11,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-61c83a429e2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_secuential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# plt.plot(N, time_parallel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time Matrix Multiplication\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2759\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2761\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \"\"\"\n\u001b[0;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1647\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1648\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (6,) and (11,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = [1,2,4,8,16,32]\n",
    "time_secuential = [0.004480, 0.011200, 0.050400, 0.186400, 0.532160, 0.526080, 3.874560, 4.172800 ,4.177920 ,4.188160, 3.379200]\n",
    "#time_secuential = [1.102950,  1.282140,  1.464412,  1.876613,  2.856960,  6.759979] \n",
    "time_parallel = []\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(N, time_secuential)\n",
    "# plt.plot(N, time_parallel)\n",
    "plt.title(\"Time Matrix Multiplication\")\n",
    "plt.ylabel(\"time (ms)\")\n",
    "plt.xlabel(\"N\")\n",
    "plt.legend([\"Secuential\", \"Parallel\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T06:21:08.192320Z",
     "iopub.status.busy": "2021-01-08T06:21:08.191426Z",
     "iopub.status.idle": "2021-01-08T06:21:09.244598Z",
     "shell.execute_reply": "2021-01-08T06:21:09.245032Z"
    },
    "id": "YsxrAlvFG8SZ"
   },
   "source": [
    "#### translate(\"Knit midi dress with a V-neckline, straps and matching lace detail.<br/><br/>HEIGHT OF MODEL: 177 CM. / 69.6″\")\n",
    "print (\"OVERSIZED SHIRT WITH POCKET TRF\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
